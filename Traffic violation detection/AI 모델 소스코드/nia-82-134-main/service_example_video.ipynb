{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import mmcv\n",
    "import cv2\n",
    "import re\n",
    "import mmdet\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import PIL.ImageDraw as ImageDraw\n",
    "import PIL.Image as Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "\n",
    "from collections import OrderedDict\n",
    "from PIL import ImageFont\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from tqdm import notebook\n",
    "from tqdm.notebook import tqdm\n",
    "from mmdet.apis import init_detector, inference_detector\n",
    "from torchvision import transforms, models\n",
    "\n",
    "from constants import ImageInfo, ObjectInfo, VEHICLE_LIST, \\\n",
    "    LANE_LABEL_MAP_PREV, LANE_COLOR_MAP_MODEL, VEHICLE_COLOR_MAP, NEW_SIZE, \\\n",
    "    VIOLATION_MAP, VLT_COLOR, DANGER_COLOR, NORMAL_COLOR\n",
    "from lane_detection.model import LaneSegModel\n",
    "from utils import viz_inference_result\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORE_TH = 0.7\n",
    "\n",
    "def inference(image, vehicle_model, lane_model, device):\n",
    "    image_info = ImageInfo(image, None)\n",
    "    obj_id = 0\n",
    "\n",
    "    img_array = np.array(image)\n",
    "\n",
    "    # >>> inference cars\n",
    "    bbox_result, segm_result = inference_detector(vehicle_model, img_array)\n",
    "    bboxes = np.vstack(bbox_result)\n",
    "    labels = [\n",
    "        np.full(bbox.shape[0], i, dtype=np.int32)\n",
    "        for i, bbox in enumerate(bbox_result)\n",
    "    ]\n",
    "    labels = np.concatenate(labels)\n",
    "    if isinstance(segm_result, tuple):\n",
    "        segm_result = segm_result[0]\n",
    "    segms = None\n",
    "    if segm_result is not None and len(labels) > 0:  # non empty\n",
    "        segms = mmcv.concat_list(segm_result)\n",
    "        if isinstance(segms[0], torch.Tensor):\n",
    "            segms = torch.stack(segms, dim=0).detach().cpu().numpy()\n",
    "        else:\n",
    "            segms = np.stack(segms, axis=0)\n",
    "\n",
    "    object_list = []\n",
    "    for i in range(len(labels)):\n",
    "        bbox = bboxes[i][:4]\n",
    "        score = bboxes[i][-1]\n",
    "        if score < SCORE_TH:\n",
    "            continue\n",
    "        category = VEHICLE_LIST[labels[i]]\n",
    "        segm = segms[i]\n",
    "        # remove small size objects\n",
    "        if category == \"vehicle_bike\" and bbox[3] - bbox[1] < 100:\n",
    "            continue\n",
    "        elif category != \"vehicle_bike\" and np.sum(segm) < 10000:\n",
    "            continue\n",
    "        # cv2.CHAIN_APPROX_TC89_L1, cv2.CHAIN_APPROX_TC89_KCOS, cv2.CHAIN_APPROX_SIMPLE\n",
    "        c, _ = cv2.findContours(segm.astype(np.uint8), cv2.RETR_LIST, \n",
    "                                cv2.CHAIN_APPROX_SIMPLE)\n",
    "        approx_poly = cv2.approxPolyDP(\n",
    "            c[0], 0.005 * cv2.arcLength(c[0], True), True).astype(float)\n",
    "        points = approx_poly.squeeze(1).reshape(-1)\n",
    "        object_list.append(ObjectInfo(obj_id=obj_id,\n",
    "                                      obj_type=\"vehicle\", \n",
    "                                      bbox=bbox,\n",
    "                                      poly=points,\n",
    "                                      score=score,\n",
    "                                      category=category,\n",
    "                                      segm=segm))\n",
    "        obj_id += 1\n",
    "    image_info.objects = object_list\n",
    "    result_img = vehicle_model.show_result(img_array, (bbox_result, segm_result))\n",
    "    \n",
    "    # for debug\n",
    "\n",
    "    # <<< inference cars end.\n",
    "    \n",
    "    # >>> inference lanes\n",
    "    input_img = Image\n",
    "    w, h = image.size\n",
    "    img_tensor = transforms.functional.to_tensor(\n",
    "            transforms.functional.resized_crop(\n",
    "                image, h - w // 2, 0, w // 2, w, (800, 1333)\n",
    "            )\n",
    "        )\n",
    "    img_tensor = img_tensor.to(device)\n",
    "    lane_out = torch.sigmoid(lane_model(img_tensor.unsqueeze(0))['out'])\n",
    "    final_out = torch.argmax(lane_out[0],0).view(1,1,800,1333).to(torch.float)\n",
    "    final_out = torch.nn.functional.interpolate(final_out, (1080,1920)).squeeze()\n",
    "    lane_mask = np.array(final_out.to(\"cpu\"))\n",
    "    lane_mask = lane_mask.astype(np.uint8)\n",
    "    lane_mask = Image.fromarray(lane_mask, mode=\"L\")\n",
    "    \n",
    "    # post processing\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    lane_mask = cv2.dilate(np.array(lane_mask), kernel, iterations=3)\n",
    "\n",
    "    for label in np.unique(lane_mask):\n",
    "        if label == 0:\n",
    "            continue\n",
    "        segm = np.where(np.array(lane_mask) == label, True, False)\n",
    "        if np.sum(segm) < 1500:\n",
    "            continue\n",
    "\n",
    "\n",
    "        category = LANE_LABEL_MAP_PREV[label]\n",
    "        # cv2.CHAIN_APPROX_TC89_L1, cv2.CHAIN_APPROX_TC89_KCOS, cv2.CHAIN_APPROX_SIMPLE\n",
    "        c, _ = cv2.findContours(segm.astype(np.uint8), cv2.RETR_LIST, \n",
    "                                cv2.CHAIN_APPROX_SIMPLE)\n",
    "        # TODO: deal with more than one contour\n",
    "        bbox = cv2.boundingRect(c[0])\n",
    "        bbox = (bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3])\n",
    "        approx_poly = cv2.approxPolyDP(c[0], 0.005 * cv2.arcLength(c[0], True), \n",
    "                                       True)\n",
    "        points = approx_poly.squeeze(1).reshape(-1).astype(float)\n",
    "        obj_info = ObjectInfo(obj_id, \"lane\", category, \n",
    "                              np.array(bbox), points, segm, 1.)\n",
    "        obj_id += 1\n",
    "        image_info.objects.append(obj_info)\n",
    "    # <<< end.\n",
    "\n",
    "    return image_info\n",
    "\n",
    "def detect_violation(object_info, violation_model, device):\n",
    "    def _get_arr_mask(pil_img, new_size):\n",
    "        result_arr = np.array(pil_img.copy().resize(new_size, Image.NEAREST))\n",
    "        result_arr = np.mean(result_arr, axis=2)\n",
    "        result_arr = np.where(result_arr > 0, True, False)\n",
    "        \n",
    "        return result_arr\n",
    "    \n",
    "    lane_list = []\n",
    "    car_list = []\n",
    "    result_list = []\n",
    "    object_info = sorted(object_info, key=lambda k: np.sum(k.segm))\n",
    "    for obj in object_info:\n",
    "        if obj.obj_type == \"vehicle\":\n",
    "            car_list.append(obj)\n",
    "        else:\n",
    "            lane_list.append(obj)\n",
    "            result_list.append(obj)\n",
    "    \n",
    "    img_size = (1920, 1080)\n",
    "    \n",
    "    car_arr_list = []\n",
    "    for car in car_list:\n",
    "        # get array intersection is filled\n",
    "        car_ctg = car.category\n",
    "        result_img = Image.new(\"RGB\", img_size)\n",
    "        result_img_arr = np.array(result_img)\n",
    "        result_img_arr[car.segm, :] = VEHICLE_COLOR_MAP[car_ctg]\n",
    "        car_arr_mask = _get_arr_mask(Image.fromarray(result_img_arr), NEW_SIZE)\n",
    "        intersections = []\n",
    "        match = []\n",
    "        for lane in lane_list:\n",
    "            lane_ctg = lane.category\n",
    "            result_img_arr[lane.segm] = LANE_COLOR_MAP_MODEL[lane.category]\n",
    "            result_img = Image.fromarray(result_img_arr)\n",
    "            blank_img = Image.new(\"RGB\", img_size)\n",
    "            blank_img_arr = np.array(blank_img)\n",
    "            blank_img_arr[lane.segm] = LANE_COLOR_MAP_MODEL[lane.category]\n",
    "            lane_arr_mask = _get_arr_mask(Image.fromarray(blank_img_arr), \n",
    "                                         NEW_SIZE)\n",
    "            if np.sum(car_arr_mask & lane_arr_mask) > 10:\n",
    "                match = [car.obj_id, lane.obj_id]\n",
    "            \n",
    "            if lane_ctg in VIOLATION_MAP[car_ctg]:\n",
    "                color = VLT_COLOR\n",
    "            elif lane_ctg in VIOLATION_MAP[\"danger\"]:\n",
    "                color = DANGER_COLOR\n",
    "            else:\n",
    "                color = NORMAL_COLOR\n",
    "            intersections.append((color, \n",
    "                                  car_arr_mask & lane_arr_mask, \n",
    "                                  match))\n",
    "        result_img_arr = np.array(result_img.resize(NEW_SIZE, Image.NEAREST))\n",
    "        for inter in intersections:\n",
    "            result_img_arr[inter[1], :] = inter[0]\n",
    "        result_img_arr = result_img_arr.astype(np.float32)\n",
    "        result_img_arr /= 255.\n",
    "        car_arr_list.append((result_img_arr, match))\n",
    "        # plt.imshow(result_img_arr)\n",
    "        # plt.show()\n",
    "    \n",
    "    # Inference model\n",
    "    if len(car_arr_list) == 0:\n",
    "        return result_list\n",
    "    car_arr_list = np.array(car_arr_list, dtype=object)\n",
    "    input_image = np.stack(\n",
    "        car_arr_list[:, 0]).transpose(0, 3, 1, 2)\n",
    "    input_image = torch.from_numpy(input_image).type(torch.float32)\n",
    "    data_transforms = torch.nn.Sequential(\n",
    "            transforms.Resize(224),\n",
    "            transforms.Normalize(\n",
    "                        [0.0181, 0.0304, 0.0147], [0.1199, 0.1648, 0.1065]\n",
    "                    ),\n",
    "        )\n",
    "    img_tensor = data_transforms(input_image)\n",
    "    img_tensor = img_tensor.to(device)\n",
    "    outputs = violation_model(img_tensor)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    preds = np.array(preds.to(\"cpu\"))\n",
    "    \n",
    "    if 2 in preds:\n",
    "        car_idx = np.where(preds == 2)[0][0]\n",
    "        vlt_car = car_list[car_idx]\n",
    "        vlt_car.label = \"violation\"\n",
    "        match = car_arr_list[car_idx][-1]\n",
    "        if len(match) > 0: \n",
    "            lane_obj_id = match[-1]\n",
    "            new_list = [vlt_car]\n",
    "            for lane in result_list:\n",
    "                if lane.obj_id == lane_obj_id:\n",
    "                    new_list.append(lane)\n",
    "            result_list = new_list\n",
    "        else:\n",
    "            result_list.append(vlt_car)\n",
    "    elif 0 in preds:\n",
    "        car_idx = np.where(preds == 0)[0][0]\n",
    "        vlt_car = car_list[car_idx]\n",
    "        vlt_car.label = \"danger\"\n",
    "        match = car_arr_list[car_idx][-1]\n",
    "        if len(match) > 0: \n",
    "            lane_obj_id = match[-1]\n",
    "            new_list = [vlt_car]\n",
    "            for lane in result_list:\n",
    "                if lane.obj_id == lane_obj_id:\n",
    "                    new_list.append(lane)\n",
    "            result_list = new_list\n",
    "        else:\n",
    "            result_list.append(vlt_car)\n",
    "    else:\n",
    "        for car in car_list[:3]:\n",
    "            result_list.append(car)\n",
    "    \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config 파일 지정 및 학습된 모델 경로 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_cfg_path = './configs/vehicle_detection_config.py'\n",
    "vehicle_ckpt_path = \"./best_models/vehicle_detection_model.pth\"\n",
    "\n",
    "lane_ckpt_path = \"./best_models/lane_all.pth\"\n",
    "\n",
    "violation_ckpt_path = \"./best_models/vlt_cls_model.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델들 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: ./best_models/vehicle_detection_model.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vehicle_model = init_detector(vehicle_cfg_path, vehicle_ckpt_path, device=device)\n",
    "\n",
    "lane_model = LaneSegModel(num_classes=13)\n",
    "lane_model_ckpt = torch.load(lane_ckpt_path, map_location=device)\n",
    "new_state_dict = OrderedDict()\n",
    "for n, v in lane_model_ckpt[\"state_dict\"].items():\n",
    "    new_name = n.replace(\"module.\", \"\")\n",
    "    new_state_dict[new_name] = v\n",
    "lane_model.load_state_dict(new_state_dict)\n",
    "lane_model.to(device)\n",
    "lane_model.eval()\n",
    "\n",
    "violation_model = models.resnet18(pretrained=False)\n",
    "num_ftrs = violation_model.fc.in_features\n",
    "violation_model.fc = nn.Linear(num_ftrs, 3)\n",
    "violation_model_ckpt = torch.load(violation_ckpt_path, map_location=device)\n",
    "violation_model.load_state_dict(violation_model_ckpt[\"state_dict\"])\n",
    "violation_model.to(device)\n",
    "violation_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576068c5305544e0b462c8fe209ffb17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Video:   0%|          | 0/9000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/nia-82-134/lib/python3.8/site-packages/mmdet/datasets/utils.py:65: UserWarning: \"ImageToTensor\" pipeline is replaced by \"DefaultFormatBundle\" for batch inference. It is recommended to manually replace it in the test data pipeline in your config file.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 비디오 파일을 불러옵니다.\n",
    "video_path = 'road_sample_8.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# 비디오의 프레임 크기 및 FPS를 얻습니다.\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# 결과 동영상을 저장할 객체를 초기화합니다.\n",
    "output_path = 'road_result_8.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v') # 'mp4v'는 MP4 코덱을 의미합니다.\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# tqdm 진행 바를 초기화합니다.\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "pbar = tqdm(total=total_frames, desc='Processing Video')\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # 현재 프레임을 PIL 이미지로 변환\n",
    "    curr_img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    # 인퍼런스 수행\n",
    "    image_info = inference(curr_img, vehicle_model, lane_model, device)\n",
    "    \n",
    "    # 위반 감지\n",
    "    result_list = detect_violation(image_info.objects, violation_model, device)\n",
    "    image_info.objects = result_list\n",
    "    \n",
    "    # 결과 시각화\n",
    "    result_img = viz_inference_result(curr_img, image_info)\n",
    "    result_cv_img = cv2.cvtColor(np.array(result_img), cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # 결과 프레임을 동영상에 추가\n",
    "    out.write(result_cv_img)\n",
    "    \n",
    "    # 진행 바 업데이트\n",
    "    pbar.update(1)\n",
    "\n",
    "# 진행 바 종료\n",
    "pbar.close()\n",
    "\n",
    "# 모든 객체 해제\n",
    "cap.release()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python (ipykernel)",
   "language": "python",
   "name": "nia-82-134"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
