{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63808012-2748-4957-9f63-5b4dc5d08b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class LaneDataset(Dataset):\n",
    "    def __init__(self, root, split=\"train\"):\n",
    "        if split == \"train\":\n",
    "            self.I_H = 800\n",
    "            self.I_W = 1333\n",
    "        else:\n",
    "            self.I_H = 800\n",
    "            self.I_W = 1333\n",
    "\n",
    "        self.img_list = glob(os.path.join(root, split, \"IMAGE/*.jpg\"))\n",
    "        self.label_path = [\n",
    "            i.replace(\"IMAGE\", \"ANNOTATION\").replace(\"jpg\", \"json\")\n",
    "            for i in self.img_list\n",
    "        ]\n",
    "\n",
    "        self.len = len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        while True:\n",
    "            img_path = self.img_list[index]\n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                break\n",
    "            except:\n",
    "                with open(\"error_files.txt\", \"a\") as errlog:\n",
    "                    errlog.write(str(index) + \": \" + img_path + \"\\n\")\n",
    "                    index = index + 1\n",
    "\n",
    "        w, h = img.size\n",
    "        label_path = self.label_path[index]\n",
    "        with open(label_path, \"r\") as f:\n",
    "            json_data = json.load(f)\n",
    "        img_tensor = transforms.functional.to_tensor(\n",
    "            transforms.functional.resized_crop(\n",
    "                img, h - w // 2, 0, w // 2, w, (self.I_H, self.I_W)\n",
    "            )\n",
    "        )\n",
    "        target_map = self.make_gt_map(json_data, w, h)\n",
    "\n",
    "        return img_tensor, torch.LongTensor(target_map), img_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def make_gt_map(self, json_data, original_w, original_h):\n",
    "\n",
    "        target_map = np.zeros((self.I_H, self.I_W), dtype=np.int32)\n",
    "        annotation = json_data[\"data_set_info\"][\"data\"]\n",
    "        y_offset = original_h - original_w // 2\n",
    "\n",
    "        for item in annotation:\n",
    "            label = item[\"value\"][\"object_Label\"]\n",
    "            if \"lane_type\" in label.keys():\n",
    "                obj_class = label[\"lane_type\"]\n",
    "                obj_lab_att = label[\"lane_attribute\"]\n",
    "            else:\n",
    "                continue\n",
    "            if obj_class[5:] == \"white\":\n",
    "                pos = item[\"value\"][\"points\"]\n",
    "                poly_points = np.array(\n",
    "                    [\n",
    "                        (\n",
    "                            [\n",
    "                                pt[\"x\"] * self.I_W / original_w,\n",
    "                                (pt[\"y\"] - y_offset)\n",
    "                                * self.I_H\n",
    "                                / (original_h - y_offset),\n",
    "                            ]\n",
    "                        )\n",
    "                        for pt in pos\n",
    "                    ]\n",
    "                ).astype(np.int32)\n",
    "                if obj_lab_att == \"single_solid\":\n",
    "                    cv2.fillPoly(target_map, [poly_points], 1)\n",
    "                elif obj_lab_att == \"single_dashed\":\n",
    "                    cv2.fillPoly(target_map, [poly_points], 1)\n",
    "                elif obj_lab_att == \"double_solid\":\n",
    "                    cv2.fillPoly(target_map, [poly_points], 1)\n",
    "                elif obj_lab_att == \"left_dashed_double\":\n",
    "                    cv2.fillPoly(target_map, [poly_points], 1)\n",
    "                elif obj_lab_att == \"right_dashed_double\":\n",
    "                    cv2.fillPoly(target_map, [poly_points], 1)\n",
    "\n",
    "            elif obj_class[5:] == \"blue\":\n",
    "                pos = item[\"value\"][\"points\"]\n",
    "                poly_points = np.array(\n",
    "                    [\n",
    "                        (\n",
    "                            [\n",
    "                                pt[\"x\"] * self.I_W / original_w,\n",
    "                                (pt[\"y\"] - y_offset)\n",
    "                                * self.I_H\n",
    "                                / (original_h - y_offset),\n",
    "                            ]\n",
    "                        )\n",
    "                        for pt in pos\n",
    "                    ]\n",
    "                ).astype(np.int32)\n",
    "                if obj_lab_att == \"single_solid\":\n",
    "                    cv2.fillPoly(target_map, [poly_points], 2)\n",
    "                elif obj_lab_att == \"single_dashed\":\n",
    "                    cv2.fillPoly(target_map, [poly_points], 2)\n",
    "                elif obj_lab_att == \"double_solid\":\n",
    "                    cv2.fillPoly(target_map, [poly_points], 2)\n",
    "                elif obj_lab_att == \"left_dashed_double\":\n",
    "                    cv2.fillPoly(target_map, [poly_points], 2)\n",
    "                elif obj_lab_att == \"right_dashed_double\":\n",
    "                    cv2.fillPoly(target_map, [poly_points], 2)\n",
    "\n",
    "            elif obj_class[5:] == \"yellow\":\n",
    "                pos = item[\"value\"][\"points\"]\n",
    "                poly_points = np.array(\n",
    "                    [\n",
    "                        (\n",
    "                            [\n",
    "                                pt[\"x\"] * self.I_W / original_w,\n",
    "                                (pt[\"y\"] - y_offset)\n",
    "                                * self.I_H\n",
    "                                / (original_h - y_offset),\n",
    "                            ]\n",
    "                        )\n",
    "                        for pt in pos\n",
    "                    ]\n",
    "                ).astype(np.int32)\n",
    "                if obj_lab_att == \"single_solid\":\n",
    "                    cv2.fillPoly(target_map, [poly_points], 3)\n",
    "                elif obj_lab_att == \"single_dashed\":\n",
    "                    cv2.fillPoly(target_map, [poly_points], 3)\n",
    "                elif obj_lab_att == \"double_solid\":\n",
    "                    cv2.fillPoly(target_map, [poly_points], 3)\n",
    "                elif obj_lab_att == \"left_dashed_double\":\n",
    "                    cv2.fillPoly(target_map, [poly_points], 3)\n",
    "                elif obj_lab_att == \"right_dashed_double\":\n",
    "                    cv2.fillPoly(target_map, [poly_points], 3)\n",
    "\n",
    "            elif obj_class[5:] == \"shoulder\":\n",
    "                pos = item[\"value\"][\"points\"]\n",
    "                poly_points = np.array(\n",
    "                    [\n",
    "                        (\n",
    "                            [\n",
    "                                pt[\"x\"] * self.I_W / original_w,\n",
    "                                (pt[\"y\"] - y_offset)\n",
    "                                * self.I_H\n",
    "                                / (original_h - y_offset),\n",
    "                            ]\n",
    "                        )\n",
    "                        for pt in pos\n",
    "                    ]\n",
    "                ).astype(np.int32)\n",
    "                if obj_lab_att == \"single_solid\":\n",
    "                    cv2.fillPoly(target_map, [poly_points], 4)\n",
    "                elif obj_lab_att == \"single_dashed\":\n",
    "                    cv2.fillPoly(target_map, [poly_points], 4)\n",
    "                elif obj_lab_att == \"double_solid\":\n",
    "                    cv2.fillPoly(target_map, [poly_points], 4)\n",
    "                elif obj_lab_att == \"left_dashed_double\":\n",
    "                    cv2.fillPoly(target_map, [poly_points], 4)\n",
    "                elif obj_lab_att == \"right_dashed_double\":\n",
    "                    cv2.fillPoly(target_map, [poly_points], 4)\n",
    "\n",
    "        return target_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5be74ff4-b942-42c2-a274-fa60828603ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# class FocalLoss(nn.Module):\n",
    "#     def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "#         super(FocalLoss, self).__init__()\n",
    "#         self.gamma = gamma\n",
    "#         self.alpha = alpha\n",
    "#         if isinstance(alpha, (float, int)):\n",
    "#             self.alpha = torch.Tensor([alpha, 1 - alpha])\n",
    "#         if isinstance(alpha, list):\n",
    "#             self.alpha = torch.Tensor(alpha)\n",
    "#         self.size_average = size_average\n",
    "\n",
    "#     def forward(self, inp, target):\n",
    "#         if inp.dim() > 2:\n",
    "#             inp = inp.view(inp.size(0), inp.size(1), -1)  # N,C,H,W => N,C,H*W\n",
    "#             inp = inp.transpose(1, 2)  # N,C,H*W => N,H*W,C\n",
    "#             inp = inp.contiguous().view(-1, inp.size(2))  # N,H*W,C => N*H*W,C\n",
    "#         target = target.view(-1, 1)\n",
    "\n",
    "#         logpt = F.log_softmax(inp)\n",
    "#         logpt = logpt.gather(1, target)\n",
    "#         logpt = logpt.view(-1)\n",
    "#         pt = Variable(logpt.data.exp())\n",
    "\n",
    "#         if self.alpha is not None:\n",
    "#             if self.alpha.type() != inp.data.type():\n",
    "#                 self.alpha = self.alpha.type_as(inp.data)\n",
    "#             at = self.alpha.gather(0, target.data.view(-1))\n",
    "#             logpt = logpt * Variable(at)\n",
    "\n",
    "#         loss = -1 * (1 - pt) ** self.gamma * logpt\n",
    "#         if self.size_average:\n",
    "#             return loss.mean()\n",
    "#         else:\n",
    "#             return loss.sum()\n",
    "\n",
    "\n",
    "class FocalLoss(nn.modules.loss._WeightedLoss):\n",
    "    def __init__(self, weight=None, gamma=2, reduction=\"mean\"):\n",
    "        super(FocalLoss, self).__init__(weight, reduction=reduction)\n",
    "        self.gamma = gamma\n",
    "        # weight parameter will act as the alpha parameter to balance class weights\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, input, target):\n",
    "\n",
    "        ce_loss = F.cross_entropy(\n",
    "            input, target, reduction=self.reduction, weight=self.weight\n",
    "        )\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n",
    "        return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6862ebe5-d0e0-4f39-9b81-ddef073e1cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torchvision.models.segmentation import fcn_resnet50\n",
    "\n",
    "\n",
    "class LaneSegModel(nn.Module):\n",
    "    def __init__(self, num_classes=21):\n",
    "        super(LaneSegModel, self).__init__()\n",
    "        self.fcn = fcn_resnet50(pretrained=True)\n",
    "        in_channels = 2048\n",
    "        inter_channels = in_channels // 4\n",
    "        channels = num_classes\n",
    "        self.num_lanes = num_classes\n",
    "        self.fcn.classifier = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(inter_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv2d(inter_channels, channels, 1),  # yellow, white, blue, shoulder\n",
    "        )\n",
    "        self.f1 = 0\n",
    "        self.f1cnt = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fcn(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4678bb41-938f-45fa-8ac4-38c7b7ede9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "def test_step(\n",
    "    batch, model, device, batch_idx, batch_size, show=False, file_output=False\n",
    "):\n",
    "    x, y, img_path = batch\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    out = torch.sigmoid(model(x)[\"out\"])\n",
    "    confusion_mat = torch.zeros(\n",
    "        (model.num_lanes, model.num_lanes), device=device, dtype=torch.long\n",
    "    )\n",
    "    f1_sum = 0\n",
    "    f1_cnt = 0\n",
    "    acc = torch.tensor(0.0, device=device)\n",
    "    imshow = show\n",
    "    if imshow:\n",
    "        for i, output in enumerate(out):\n",
    "            final_out = torch.argmax(output, 0)\n",
    "            img = x[i].cpu().permute((1, 2, 0)).numpy()\n",
    "            # img = img[:,:,::-1]\n",
    "            plt.imsave(\"input.png\", img)\n",
    "            plt.imsave(\n",
    "                \"output.png\", (final_out.cpu()).int(), vmin=0, vmax=model.num_lanes - 1\n",
    "            )\n",
    "            plt.imsave(\n",
    "                \"target.png\", (y[i].cpu()).int(), vmin=0, vmax=model.num_lanes - 1\n",
    "            )\n",
    "            input()\n",
    "    else:\n",
    "        for i, output in enumerate(out):\n",
    "            #             print(output.shape)\n",
    "            final_out = torch.argmax(output, 0)\n",
    "            #             print(final_out.shape)\n",
    "\n",
    "            acc += torch.sum((final_out == y[i])) / (512 * 1024.0)\n",
    "\n",
    "            for xx in torch.arange(model.num_lanes, device=device):\n",
    "                for yy in torch.arange(model.num_lanes, device=device):\n",
    "                    confusion_mat[xx, yy] += torch.sum((final_out == xx) * (y[i] == yy))\n",
    "\n",
    "            aa, bb, cnt = 0, 0, 0\n",
    "            for ii in range(model.num_lanes):\n",
    "                if (\n",
    "                    torch.sum(confusion_mat[ii, :]) != 0\n",
    "                    and torch.sum(confusion_mat[:, ii]) != 0\n",
    "                ):\n",
    "                    aa += (\n",
    "                        confusion_mat[ii, ii] / torch.sum(confusion_mat[ii, :]).float()\n",
    "                    )\n",
    "                    bb += (\n",
    "                        confusion_mat[ii, ii] / torch.sum(confusion_mat[:, ii]).float()\n",
    "                    )\n",
    "                    cnt += 1\n",
    "            aa /= cnt\n",
    "            bb /= cnt\n",
    "            # self.f1 += (2*aa*bb/(aa+bb))\n",
    "            # self.f1cnt += 1\n",
    "            f1 = (2 * aa * bb / (aa + bb)).item()\n",
    "            f1_sum += f1\n",
    "            f1_cnt += 1\n",
    "            #             print(img_path[i], \"F1 measure :\", f1)\n",
    "\n",
    "            #             file_output = False\n",
    "            if file_output:\n",
    "                if not os.path.exists(\"./outputs/\"):\n",
    "                    os.mkdir(\"./outputs/\")\n",
    "                img = x[i].cpu().permute((1, 2, 0)).numpy()\n",
    "                folder_path = \"./outputs/\" + str(batch_idx * batch_size + i)\n",
    "                #                 print(folder_path)\n",
    "                if not os.path.exists(folder_path):\n",
    "                    os.mkdir(folder_path)\n",
    "                plt.imsave(folder_path + \"/input.png\", img)\n",
    "                plt.imsave(\n",
    "                    folder_path + \"/output.png\",\n",
    "                    (final_out.cpu()).int(),\n",
    "                    vmin=0,\n",
    "                    vmax=model.num_lanes - 1,\n",
    "                )\n",
    "                plt.imsave(\n",
    "                    folder_path + \"/target.png\",\n",
    "                    (y[i].cpu()).int(),\n",
    "                    vmin=0,\n",
    "                    vmax=model.num_lanes - 1,\n",
    "                )\n",
    "        acc /= len(out)\n",
    "\n",
    "        return confusion_mat.cpu().numpy(), f1_sum, f1_cnt, img_path\n",
    "\n",
    "\n",
    "def test_epoch_end(outputs):\n",
    "    sum_confusion_mat = 0\n",
    "    total_f1 = 0\n",
    "    total_f1_cnt = 0\n",
    "    for confusion_mat, f1_sum, f1_cnt, _ in outputs:\n",
    "        sum_confusion_mat += confusion_mat\n",
    "        total_f1 += f1_sum\n",
    "        total_f1_cnt += f1_cnt\n",
    "\n",
    "    #     print(\"total_f1_cnt\",total_f1_cnt)\n",
    "    #     print(\"average F1 measure\", total_f1/total_f1_cnt)\n",
    "    #     print(\"total confusion matrix:\\n\", sum_confusion_mat.cpu().numpy())\n",
    "    return total_f1 / total_f1_cnt, sum_confusion_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbbe2ae-f5b8-4c51-a369-a64e7dbc2e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
