{% extends 'base.html' %}

{% block content %}

<article class="markdown-body entry-content container-lg" itemprop="text"><h1 tabindex="-1" dir="auto"><a id="user-content-monodepth2-digging-into-self-supervised-monocular-depth-estimation" class="anchor" aria-hidden="true" tabindex="-1" href="#monodepth2-digging-into-self-supervised-monocular-depth-estimation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Monodepth2, Digging Into Self-Supervised Monocular Depth Estimation</h1>
<ul dir="auto">
<li>paper : <a href="https://arxiv.org/pdf/1806.01260.pdf" rel="nofollow">https://arxiv.org/pdf/1806.01260.pdf</a></li>
<li>github : <a href="https://github.com/nianticlabs/monodepth2">https://github.com/nianticlabs/monodepth2</a></li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-요약" class="anchor" aria-hidden="true" tabindex="-1" href="#요약"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>요약</h2>
<p dir="auto">Monodepth2는 단안(monocular) 이미지에서 깊이를 추정하는 자기 지도 학습(self-supervision) 기반의 방법임. 인코더-디코더 구조를 사용하며, 재투영 손실(Reprojection loss), 자동 마스킹(Auto-masking), 엣지 인식 스무스니스 손실(Edge-Aware Smoothness loss)이라는 새로운 기법들을 통해 깊이 추정의 정확도를 향상시킴. KITTI 데이터셋에서의 테스트 결과, Monodepth2는 기존 방법들에 비해 우수한 성능을 보였으며, 자율 주행과 로봇 공학 등 다양한 분야에 응용될 가능성을 제시함.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-목표와-도전-과제" class="anchor" aria-hidden="true" tabindex="-1" href="#목표와-도전-과제"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>목표와 도전 과제</h2>
<p dir="auto"><strong>[목표]</strong></p>
<ul dir="auto">
<li>단안 카메라 이미지만을 사용하여 깊이를 추정(다른 센서와 같은 추가적인 하드웨어 없이 3차원 환경을 이해할 수 있도록 연구)</li>
<li>자율 주행 자동차, 로봇공학, 증강 현실 등 분야에서 응용을 목표로 함</li>
</ul>
<p dir="auto"><strong>[도전과제]</strong></p>
<ol dir="auto">
<li>스케일 불변성 (Scale Ambiguity) : 단안(mono) 이미지에서 물체의 실제 크기를 알기 어려움. 스테레오(stereo) 시스템과 달리 단안(mono) 시스템은 물체까지 절대적인 거리를 직접적으로 측정할 수 없음. 이로 인해 추정된 깊이의 스케일이 실제 환경과 다를 수 있으며, 이를 해결하기 위한 방법이 필요함</li>
<li>동적 객체 (Dynamic Objects) : 움직이는 물체는 주변 환경에 비해 상대적으로 다른 움직임 패턴을 보여줌. 이는 단안(mono) 깊이 추정 모델에 혼선을 줄 수 있으며, 동적 객체를 처리하는게 큰 도전임.</li>
<li>텍스처가 없는 영역 (Textureless Regions) : 매끄럽고 텍스처가 없는 영역은 깊이 정보를 추출하기 어렵게 만듬. 이러한 영역은 깊이 추정 시, 노이즈를 발생시킬 수 있음.</li>
<li>재투영 불일치(Reprojection Inconsistencies) : 단안 시스템에서는 다른 시점의 이미지로부터 깊이를 추론할 때, 재투영(reprojection) 과정에서 불일치가 발생할 수 있음. 이는 가려진 영역(occlusions) 또는 반사(reflections)와 같은 복잡한 장면 특성 때문임.</li>
<li>일관성 유지(Consistency Maintenance) : 시간에 따라 캡처된 연속적인 이미지들 사이에서 깊이의 일관성을 유지하는 것은 어려운 작업임.</li>
</ol>
<h2 tabindex="-1" dir="auto"><a id="user-content-방법론" class="anchor" aria-hidden="true" tabindex="-1" href="#방법론"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>방법론</h2>
<p dir="auto">Monodepth2는 단안 깊이 추정(Monocular depth estimation)을 위한 자기 지도 학습(self-supervision)방법을 발전 시킨 연구로써 몇 가지 핵심 기술을 도입하여 단안 이미지로부터 깊이 정보를 추정하는 성능을 향상 시킴</p>
<ol dir="auto">
<li>재투영 손실(Reprojection loss)<br>
여러 시점에서 캡쳐된 이미지들을 이용하여 한 이미지에서 다른 이미지로 픽셀을 재투영할 때 일관성을 측정함.<br>
재투영 손실은 모델이 다른 뷰에서 보이는 같은 장면의 픽셀을 정확하게 매핑할 수 있도록 도와줌</li>
<li>자동 마스킹(Auto-Masking)<br>
카메라 또는 물체 움직임이 없는 경우, 깊이 추정이 어려워짐.<br>
그래서 자동 마스킹 기능이 이러한 정지된 프레임을 자동으로 감지하여 깊이 학습에서 제외 시킴.</li>
<li>다중 스케일 매칭(Multi-Scale Matching)<br>
깊이 추정은 다양한 스케일에서 일관성을 유지해야함. 그러므로 다중 스케일에서 외관 일치를 측정하여 깊이 추정 정확도를 높임.</li>
<li>최소 재투영 손실(Minimum Reprojection loss)<br>
재투영 후보 중에서 가장 낮은 손실을 갖는 것을 선택하여 가려짐이나 동적 객체로 인한 잠재적인 오류를 최소화 시킴.</li>
<li>깊이와 포즈 네트워크(Depth and Pose Networks)<br>
깊이 추정을 위한 네트워크(Depth Network)와, 이미지 시퀀스 간의 카메라 움직임(Pose Network)을 추정하기 위한 두 네트워크를 사용함.</li>
<li>엣지 인식 손실(Edge-Aware Smoothness loss)<br>
깊이 맵 연속성을 유지하면서도 엣지 부분에서는 급격한 깊이 변화를 허용하는 손실 함수를 적용하여, 엣지를 유지하면서 전체적인 매끄러움을 개선함</li>
</ol>
<h2 tabindex="-1" dir="auto"><a id="user-content-모델-아키텍처" class="anchor" aria-hidden="true" tabindex="-1" href="#모델-아키텍처"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>모델 아키텍처</h2>
<p dir="auto">효율적인 단안 깊이 추정(Monocular depth estimation)을 위해 깊이 네트워크(Depth Networ)와 포즈 네트워크(Pose Network)를 사용함.<br>
두 네트워크는 각각 이미지로부터 깊이 맵을 예측하고, 연속된 이미지 프레임 사이 카메라 또는 물체 움직임(포즈)을 추정하는 역할을 함.</p>
<p dir="auto"><strong>1. 깊이 네트워크(Depth Network)</strong></p>
<ul dir="auto">
<li>U-Net 아키텍처로 인코더-디코더 구조를 가지고 있음</li>
<li>인코더
<ul dir="auto">
<li>이미지로부터 고차원 특징을 추출하는 역할을 하며, pretrain된 ResNet을 사용함.</li>
</ul>
</li>
<li>디코더
<ul dir="auto">
<li>인코더에서 추출된 특징을 바탕으로 픽셀 단위의 깊이 맵을 생성함. 이 과정에서 skip connections(스킵 연결) 사용하여 인코더의 저차원 특징과 디코더의 고차원 특징을 결합함으로써, 세부적인 텍스처와 엣지 정보를 보존함.</li>
<li>디코더 출력은 다중 스케일의 깊이 맵이며, 깊이 추정을 더욱 정밀하게 말들기 위해 사용됨.
<a target="_blank" rel="noopener noreferrer" href="https://i.imgur.com/jMZzSNT.png"><img src="https://i.imgur.com/jMZzSNT.png" alt="Untitled (8)" style="max-width: 100%;"></a></li>
</ul>
</li>
</ul>
<p dir="auto"><strong>2. 포즈 네트워크(Pose Network)</strong></p>
<ul dir="auto">
<li>연속된 이미지 프레임 사이의 카메라 또는 물체 움직임을 추정함.</li>
<li>ResNet18로 구성되어있고, 여러 연속 프레임을 입력받아 카메라 이동(포즈)을 6DoF(Degree of Freedom, 3축의 회전과 3축의 이동)로 예측함.</li>
<li>포즈 네트워크의 출력은 깊이 네트워크와 함께 사용되어 재투영 손실(Reprojection loss)을 계산하는데 필요한 정보를 제공함.
<a target="_blank" rel="noopener noreferrer" href="https://i.imgur.com/xLHhxMN.png"><img src="https://i.imgur.com/xLHhxMN.png" alt="Untitled (9)" style="max-width: 100%;"></a></li>
</ul>
<p dir="auto">두 네트워크는 자기 지도 학습(self-supervision)을 통해 동시에 훈련되며, 이 과정에서 재투영 손실(Reprojection loss), 자동 마스킹(Auto-Masking), 최소 재투영 손실(Minimum Reprojection loss), 엣지 인식 손실(Edge-Aware Smoothness loss) 등 다양한 손실 함수를 적용하여 깊이를 잘 추정하고자 함</p>
<p dir="auto"><strong>3. 학습과정</strong><br>
<a target="_blank" rel="noopener noreferrer" href="https://i.imgur.com/FUTc0Ce.png"><img src="https://i.imgur.com/FUTc0Ce.png" alt="Untitled (10)" style="max-width: 100%;"></a>
(a)인 Depth 네트워크에서 Depth 추정하고 (b)인 Pose 네트워크에서 6-DoF 상대 위치를 추정함. 최종적으로 (c)인 appearance loss 에 두 추정 값이 사용됨</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-데이터셋" class="anchor" aria-hidden="true" tabindex="-1" href="#데이터셋"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>데이터셋</h2>
<ul dir="auto">
<li>KITTI Eigen Split
<ul dir="auto">
<li>KITTI 데이터셋에서 특정한 방식으로 훈련, 검증, 테스트 세트를 분할한 데이터로 Eigen et al.이 제안한 분할 방식을 따름.</li>
<li>이전 연구에서도 사용된 표준적인 방식임.</li>
</ul>
</li>
<li>Pre-processing
<ul dir="auto">
<li>Zhou et al.의 방법을 따라 정지 프레임을 제거.</li>
<li>정지 프레임은 주변 환경 변화없이 카메라만 움직인 경우를 의미하며, 이러한 프레임들은 깊이 학습에 부정적인 영향을 미칠 수 있음 그러므로 제거!!</li>
</ul>
</li>
<li>Monocular and Stereo Data
<ul dir="auto">
<li>Monodepth2는 단안(monocular)데이터와 스테레오(stereo)데이터를 모두 사용할 수 있음.</li>
<li>단안 데이터만을 사용하는 경우와 스테레오 데이터를 함께 사용하는 경우(mono+stereo)를 구분하여 실험함.</li>
</ul>
</li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-평가지표-및-결과" class="anchor" aria-hidden="true" tabindex="-1" href="#평가지표-및-결과"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>평가지표 및 결과</h2>
<p dir="auto">모델 성능 평가를 위해 다음과 같은 표준적인 깊이 추정 평가 지표가 사용됨.</p>
<ol dir="auto">
<li>절대 상대 오차(Absolute Relative Error, Abs Rel)<br>
예측된 깊이와 실제 깊이 사이의 상대적 차이를 평균하여 측정함.</li>
<li>제곱 상대 오차(Squared Relative Error, Sq Rel)<br>
예측된 깊이와 실제 깊이의 차이를 제곱한 값을 사용하여 오차를 계산함.</li>
<li>루트 평균제곱 오차(Root Mean Squared Error, RMSE)<br>
예측된 깊이와 실제 깊이 사이의 오차를 제곱하여 평균한 후 제곱근을 취함.</li>
<li>RMSE 로그(RMSE log)<br>
깊이 값의 로그를 취한 후 RMSE를 계산함.</li>
<li>정확도 지표(Threshold Accuracy)<br>
예측된 깊이 값이 실제 값의 특정 배율 내에 있을 비율을 측정하고, 일반적인 임계값은 1.25, 1.25^2, 1.25^3 등으로 설정됨.</li>
</ol>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://i.imgur.com/jJnP1DE.png"><img src="https://i.imgur.com/jJnP1DE.png" alt="Quantitative results" style="max-width: 100%;"></a>
본 방법론은 monocular 방식으로 설계가 되었지만, Stereo 성능에서도 높은 결과를 달성한 것이 흥미로움.<br>
위의 결과는 post-processing 을 진행하지 않았을때의 결과임.<br>
본 결과에 post-processing까지 적용한다면 아래 표처럼 성능이 조금 상승하게 됨.<br>
<a target="_blank" rel="noopener noreferrer" href="https://i.imgur.com/PFK0o4X.png"><img src="https://i.imgur.com/PFK0o4X.png" alt="Effect of post-processing result" style="max-width: 100%;"></a></p>
<p dir="auto">그리고 본 논문에서 Ablation study 결과는 아래와 같음.<br>
<a target="_blank" rel="noopener noreferrer" href="https://i.imgur.com/9WqPqWZ.png"><img src="https://i.imgur.com/9WqPqWZ.png" alt="Ablation study" style="max-width: 100%;"></a><br>
제안한 방법론을 하나씩 적용하고 적용하지 않았을 때 엄청난 성능차이가 있는거 같아 보이진 않음</p>
<p dir="auto">다른 모델들과 비교했을때 Monodepth2가 Ground Truth와 가장 depth값이 유사한 것으로 보임
<a target="_blank" rel="noopener noreferrer" href="https://i.imgur.com/2NMD4G4.png"><img src="https://i.imgur.com/2NMD4G4.png" alt="monodepth_Result" style="max-width: 100%;"></a></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-결론" class="anchor" aria-hidden="true" tabindex="-1" href="#결론"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>결론</h2>
<ol dir="auto">
<li>Monodepth2는 기존의 self-supervision 기반의 단안 깊이 추정(monocular depth estimation) 방법보다 더 좋은 성능을 보임</li>
<li>그러나 복잡한 장면이나 빛의 반사가 심한 영역이나, 비램버트 표면(Lambertian surface, 모든 방향에서 보아도 똑같은 밝기로 보이는 표면)등에 대한 깊이 추정의 정확도가 낮을 수 있음<br>
<a target="_blank" rel="noopener noreferrer" href="https://i.imgur.com/tK3BaoR.png"><img src="https://i.imgur.com/tK3BaoR.png" alt="화면 캡처 2023-12-21 143809" style="max-width: 100%;"></a></li>
</ol>
</article>
{% endblock %}