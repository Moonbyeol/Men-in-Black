{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71a83ffe-992c-48d3-b54c-0ab7f45c3d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_hooks = [dict(type=\"NumClassCheckHook\")]\n",
    "cudnn_benchmark = True\n",
    "eval_interval = 500\n",
    "evaluation = dict(interval=eval_interval, metric=[\"bbox\", \"segm\"], save_best=\"segm_mAP\")\n",
    "dist_params = dict(backend=\"nccl\")\n",
    "log_level = \"INFO\"\n",
    "workflow = [(\"train\", 1)]\n",
    "# runner = dict(type=\"EpochBasedRunner\", max_epochs=10)\n",
    "runner = dict(type=\"IterBasedRunner\", max_iters=600000)\n",
    "checkpoint_config = dict(interval=eval_interval, max_keep_ckpts=3)\n",
    "# ******************************************************** common config\n",
    "\n",
    "# ******************************************************** schedule config\n",
    "# optimizer by mmdetection documents\n",
    "# optimizer = dict(type=\"SGD\", lr=1e-4, momentum=0.9, weight_decay=0.0001)\n",
    "optimizer = dict(type=\"Adam\", lr=0.0003, weight_decay=0.0001)\n",
    "optimizer_config = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da25f240-46d6-4dce-bef7-fa450ddfcb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_config = dict(\n",
    "    policy=\"step\",\n",
    "    warmup=\"linear\",\n",
    "    warmup_iters=500,\n",
    "    warmup_ratio=0.001,\n",
    "    step=[400000, 500000],\n",
    "    min_lr=1e-7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cdc4eee-affb-40d2-a648-6896a993c33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = \"CocoDataset\"\n",
    "data_source = \"/data/\"\n",
    "ann_source = \"/data/lane_cocostyle/\"\n",
    "classes = (\"lane_blue\", \"lane_shoulder\", \"lane_white\", \"lane_yellow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17ea5a8f-beae-4f40-9404-d808e61bf5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_norm_cfg = dict(\n",
    "    mean=[105.685, 99.015, 101.624], std=[65.58, 65.665, 67.324], to_rgb=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84d2e5e2-29d1-4666-8f4d-c97fceee6ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pipeline = [\n",
    "    dict(type=\"LoadImageFromFile\"),\n",
    "    dict(type=\"LoadAnnotations\", with_bbox=True, with_mask=True),\n",
    "    dict(type=\"Resize\", img_scale=(1333, 800), keep_ratio=True),\n",
    "    dict(type=\"RandomFlip\", flip_ratio=0.5),\n",
    "    dict(type=\"Normalize\", **img_norm_cfg),\n",
    "    dict(type=\"Pad\", size_divisor=32),\n",
    "    dict(type=\"DefaultFormatBundle\"),\n",
    "    dict(type=\"Collect\", keys=[\"img\", \"gt_bboxes\", \"gt_labels\", \"gt_masks\"]),\n",
    "]\n",
    "test_pipeline = [\n",
    "    dict(type=\"LoadImageFromFile\"),\n",
    "    dict(\n",
    "        type=\"MultiScaleFlipAug\",\n",
    "        img_scale=(1333, 800),\n",
    "        flip=False,\n",
    "        transforms=[\n",
    "            dict(type=\"Resize\", keep_ratio=True),\n",
    "            dict(type=\"RandomFlip\"),\n",
    "            dict(type=\"Normalize\", **img_norm_cfg),\n",
    "            dict(type=\"Pad\", size_divisor=32),\n",
    "            dict(type=\"ImageToTensor\", keys=[\"img\"]),\n",
    "            dict(type=\"Collect\", keys=[\"img\"]),\n",
    "        ],\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ead7e75-5b3c-4501-9771-2c9374ad3e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_set = dict(\n",
    "    type=dataset_type,\n",
    "    img_prefix=data_source + \"train/IMAGE\",\n",
    "    classes=classes,\n",
    "    ann_file=ann_source + \"train.json\",\n",
    "    pipeline=train_pipeline,\n",
    ")\n",
    "valid_data_set = dict(\n",
    "    type=dataset_type,\n",
    "    img_prefix=data_source + \"valid/IMAGE\",\n",
    "    classes=classes,\n",
    "    ann_file=ann_source + \"valid.json\",\n",
    "    pipeline=test_pipeline,\n",
    ")\n",
    "test_data_set = dict(\n",
    "    type=dataset_type,\n",
    "    img_prefix=data_source + \"test/IMAGE\",\n",
    "    classes=classes,\n",
    "    ann_file=ann_source + \"test.json\",\n",
    "    pipeline=test_pipeline,\n",
    ")\n",
    "data = dict(\n",
    "    samples_per_gpu=4,\n",
    "    workers_per_gpu=8,\n",
    "    train=train_data_set,\n",
    "    val=valid_data_set,\n",
    "    test=test_data_set,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07594dc9-6ea0-45bc-b528-eaaa4bb1d396",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dict(\n",
    "    backbone=dict(\n",
    "        depth=101,\n",
    "        init_cfg=dict(type=\"Pretrained\", checkpoint=\"torchvision://resnet101\"),\n",
    "    ),\n",
    "    neck=dict(\n",
    "        type=\"FPN_CARAFE\",\n",
    "        in_channels=[256, 512, 1024, 2048],\n",
    "        out_channels=256,\n",
    "        num_outs=5,\n",
    "        start_level=0,\n",
    "        end_level=-1,\n",
    "        norm_cfg=None,\n",
    "        act_cfg=None,\n",
    "        order=(\"conv\", \"norm\", \"act\"),\n",
    "        upsample_cfg=dict(\n",
    "            type=\"carafe\",\n",
    "            up_kernel=7,\n",
    "            up_group=1,\n",
    "            encoder_kernel=5,\n",
    "            encoder_dilation=1,\n",
    "            compressed_channels=32,\n",
    "        ),\n",
    "    ),  # CARAFE: Content-Aware ReAssembly of FEatures\n",
    "    roi_head=dict(\n",
    "        bbox_roi_extractor=dict(\n",
    "            type=\"GenericRoIExtractor\",\n",
    "            aggregation=\"sum\",\n",
    "            roi_layer=dict(type=\"RoIAlign\", output_size=7, sampling_ratio=2),\n",
    "            out_channels=256,\n",
    "            featmap_strides=[4, 8, 16, 32],\n",
    "            pre_cfg=dict(\n",
    "                type=\"ConvModule\",\n",
    "                in_channels=256,\n",
    "                out_channels=256,\n",
    "                kernel_size=5,\n",
    "                padding=2,\n",
    "                inplace=False,\n",
    "            ),\n",
    "            post_cfg=dict(\n",
    "                type=\"GeneralizedAttention\",\n",
    "                in_channels=256,\n",
    "                spatial_range=-1,\n",
    "                num_heads=6,\n",
    "                attention_type=\"0100\",\n",
    "                kv_stride=2,\n",
    "            ),\n",
    "        ),\n",
    "        mask_roi_extractor=dict(\n",
    "            type=\"GenericRoIExtractor\",\n",
    "            roi_layer=dict(type=\"RoIAlign\", output_size=14, sampling_ratio=2),\n",
    "            out_channels=256,\n",
    "            featmap_strides=[4, 8, 16, 32],\n",
    "            pre_cfg=dict(\n",
    "                type=\"ConvModule\",\n",
    "                in_channels=256,\n",
    "                out_channels=256,\n",
    "                kernel_size=5,\n",
    "                padding=2,\n",
    "                inplace=False,\n",
    "            ),\n",
    "            post_cfg=dict(\n",
    "                type=\"GeneralizedAttention\",\n",
    "                in_channels=256,\n",
    "                spatial_range=-1,\n",
    "                num_heads=6,\n",
    "                attention_type=\"0100\",\n",
    "                kv_stride=2,\n",
    "            ),\n",
    "        ),  # GRIOE A novel Region of Interest Extraction Layer for Instance Segmentation\n",
    "        bbox_head=dict(\n",
    "            num_classes=4,\n",
    "            loss_cls=dict(\n",
    "                type=\"SeesawLoss\", p=0.8, q=2.0, num_classes=4, loss_weight=1.0\n",
    "            ),\n",
    "        ),  # SeesawLoss Seesaw Loss for Long-Tailed Instance Segmentation\n",
    "        mask_head=dict(\n",
    "            num_classes=4,\n",
    "            upsample_cfg=dict(  # CARAFE: Content-Aware ReAssembly of FEatures\n",
    "                type=\"carafe\",\n",
    "                scale_factor=2,\n",
    "                up_kernel=7,  # 5 -> 7로 고침\n",
    "                up_group=1,\n",
    "                encoder_kernel=5,  # 3 -> 5로 고침\n",
    "                encoder_dilation=1,\n",
    "                compressed_channels=32,\n",
    "            ),  # 64 -> 32로 고침\n",
    "            predictor_cfg=dict(type=\"NormedConv2d\", tempearture=20),\n",
    "        ),  # SeesawLoss의 Normalized Mask Predication\n",
    "    ),\n",
    "    test_cfg=dict(\n",
    "        rpn=dict(\n",
    "            nms_pre=1000,\n",
    "            max_per_img=1000,\n",
    "            nms=dict(type=\"nms\", iou_threshold=0.7),\n",
    "            min_bbox_size=0,\n",
    "        ),\n",
    "        rcnn=dict(\n",
    "            score_thr=0.05,\n",
    "            nms=dict(type=\"nms\", iou_threshold=0.5),\n",
    "            max_per_img=100,\n",
    "            mask_thr_binary=0.5,\n",
    "        ),\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09272950-834d-4b23-99fd-60d1b5782098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import os.path as osp\n",
    "import time\n",
    "from glob import glob\n",
    "\n",
    "import mmcv\n",
    "import torch\n",
    "from mmcv import Config\n",
    "from mmcv.runner import get_dist_info, init_dist\n",
    "from mmcv.utils import get_git_hash\n",
    "from mmdet import __version__\n",
    "from mmdet.apis import set_random_seed, train_detector\n",
    "from mmdet.datasets import build_dataset\n",
    "from mmdet.models import build_detector\n",
    "from mmdet.utils import collect_env\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import get_area, setup_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a4f9f1a-b515-4893-a7be-fdc9569ee954",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--config CONFIG] [--no-validate] [--gpus GPUS | --gpu-ids GPU_IDS [GPU_IDS ...]]\n",
      "                             [--deterministic] [--launcher {none,pytorch,slurm,mpi}] [--no-merging]\n",
      "                             [--local_rank LOCAL_RANK]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\user\\AppData\\Roaming\\jupyter\\runtime\\kernel-d27c9f35-5292-457b-8b90-6fa9a8ef74d1.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\nia-82-134\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Train a vehicle detector\")\n",
    "parser.add_argument(\n",
    "    \"--config\",\n",
    "    help=\"train config file path\",\n",
    "    default=\"./configs/lane_detection_config.py\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--no-validate\",\n",
    "    action=\"store_true\",\n",
    "    help=\"whether not to evaluate the checkpoint during training\",\n",
    ")\n",
    "group_gpus = parser.add_mutually_exclusive_group()\n",
    "group_gpus.add_argument(\n",
    "    \"--gpus\",\n",
    "    type=int,\n",
    "    help=\"number of gpus to use \" \"(only applicable to non-distributed training)\",\n",
    ")\n",
    "group_gpus.add_argument(\n",
    "    \"--gpu-ids\",\n",
    "    type=int,\n",
    "    nargs=\"+\",\n",
    "    help=\"ids of gpus to use \" \"(only applicable to non-distributed training)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--deterministic\",\n",
    "    action=\"store_true\",\n",
    "    help=\"whether to set deterministic options for CUDNN backend.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--launcher\",\n",
    "    choices=[\"none\", \"pytorch\", \"slurm\", \"mpi\"],\n",
    "    default=\"none\",\n",
    "    help=\"job launcher\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--no-merging\",\n",
    "    action=\"store_true\",\n",
    "    help=\"whether to merge annotations before training. \"\n",
    "    \"Set true when there exists merged json file\",\n",
    ")\n",
    "parser.add_argument(\"--local_rank\", type=int, default=0)\n",
    "args = parser.parse_args()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5326a90-d871-4866-8f1d-fa902e6c38ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인자를 설정하기 위한 함수를 정의합니다.\n",
    "def get_args(notebook_args):\n",
    "    parser = argparse.ArgumentParser(description=\"Train a vehicle detector\")\n",
    "    parser.add_argument(\n",
    "        \"--config\",\n",
    "        help=\"train config file path\",\n",
    "        default=\"./configs/lane_detection_config.py\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--no-validate\",\n",
    "        action=\"store_true\",\n",
    "        help=\"whether not to evaluate the checkpoint during training\",\n",
    "    )\n",
    "    group_gpus = parser.add_mutually_exclusive_group()\n",
    "    group_gpus.add_argument(\n",
    "        \"--gpus\",\n",
    "        type=int,\n",
    "        help=\"number of gpus to use (only applicable to non-distributed training)\",\n",
    "    )\n",
    "    group_gpus.add_argument(\n",
    "        \"--gpu-ids\",\n",
    "        type=int,\n",
    "        nargs=\"+\",\n",
    "        help=\"ids of gpus to use (only applicable to non-distributed training)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--deterministic\",\n",
    "        action=\"store_true\",\n",
    "        help=\"whether to set deterministic options for CUDNN backend.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--launcher\",\n",
    "        choices=[\"none\", \"pytorch\", \"slurm\", \"mpi\"],\n",
    "        default=\"none\",\n",
    "        help=\"job launcher\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--no-merging\",\n",
    "        action=\"store_true\",\n",
    "        help=\"whether to merge annotations before training. \"\n",
    "        \"Set true when there exists merged json file\",\n",
    "    )\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=0)\n",
    "    \n",
    "    # parse_args 대신에 notebook_args를 사용합니다.\n",
    "    args = parser.parse_args(notebook_args)\n",
    "    return args\n",
    "\n",
    "# Jupyter 노트북에서 사용할 인자를 설정합니다.\n",
    "notebook_args = [\n",
    "    '--config', './configs/lane_detection_config.py',  # 예시 경로\n",
    "    '--no-validate',\n",
    "    '--gpus', '1',\n",
    "    '--launcher', 'none',\n",
    "    # '--no-merging',  # 이 옵션을 사용하려면 주석을 해제하세요.\n",
    "    # '--local_rank', '0',  # 이 옵션을 사용하려면 주석을 해제하고 값을 설정하세요.\n",
    "]\n",
    "\n",
    "# 인자를 가져옵니다.\n",
    "args = get_args(notebook_args)\n",
    "\n",
    "# 디바이스 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e65c9ec-d4de-4607-8b97-fd02d6f5f5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    if \"LOCAL_RANK\" not in os.environ:\n",
    "        os.environ[\"LOCAL_RANK\"] = str(args.local_rank)\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a45aabb5-8373-49b4-9803-52f94149f40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_annotation(annot_list, save_path):\n",
    "    images = []\n",
    "    annotations = []\n",
    "    annot_id = 1\n",
    "    image_id = 0\n",
    "    for annot in tqdm(annot_list):\n",
    "        with open(annot, \"r\") as f:\n",
    "            json_data = json.load(f)\n",
    "        object_data = json_data[\"data_set_info\"][\"data\"]\n",
    "        img_path = annot.replace(\"ANNOTATION\", \"IMAGE\").replace(\"json\", \"jpg\")\n",
    "        if not os.path.exists(img_path):\n",
    "            continue\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        img_info = {}\n",
    "        img_info[\"id\"] = image_id\n",
    "        file_name = osp.basename(img_path)\n",
    "        img_info[\"file_name\"] = copy.deepcopy(file_name)\n",
    "        img_info[\"height\"] = copy.deepcopy(img.size[1])\n",
    "        img_info[\"width\"] = copy.deepcopy(img.size[0])\n",
    "        images.append(copy.deepcopy(img_info))\n",
    "        img.close()\n",
    "\n",
    "        # annotation은 segmentation / iscrowd, image_id, \\\n",
    "        # category_id, id, bbox, area\n",
    "        for target in object_data:\n",
    "            obj_label = target[\"value\"][\"object_Label\"]\n",
    "            if \"lane_type\" in obj_label.keys():\n",
    "                category = obj_label[\"lane_type\"]\n",
    "            else:\n",
    "                continue\n",
    "            ann = {}\n",
    "            if category in [\n",
    "                \"lane_blue\",\n",
    "                \"lane_shoulder\",\n",
    "                \"lane_white\",\n",
    "                \"lane_yellow\",\n",
    "            ]:\n",
    "                points = target[\"value\"][\"points\"]\n",
    "                temp_points = copy.deepcopy(points)\n",
    "                # deepcopy를 해야 둘다 변경되지 않음\n",
    "                area = get_area(temp_points)\n",
    "                if category == \"lane_blue\":\n",
    "                    ann[\"category_id\"] = 1\n",
    "                elif category == \"lane_shoulder\":\n",
    "                    ann[\"category_id\"] = 2\n",
    "                elif category == \"lane_white\":\n",
    "                    ann[\"category_id\"] = 3\n",
    "                elif category == \"lane_yellow\":\n",
    "                    ann[\"category_id\"] = 4\n",
    "                segmentation = []\n",
    "                seg_x = []\n",
    "                seg_y = []\n",
    "                for point in points:\n",
    "                    segmentation.append(point[\"x\"])\n",
    "                    seg_x.append(point[\"x\"])\n",
    "                    segmentation.append(point[\"y\"])\n",
    "                    seg_y.append(point[\"y\"])\n",
    "                bbox = [\n",
    "                    min(seg_x),\n",
    "                    min(seg_y),\n",
    "                    max(seg_x) - min(seg_x),\n",
    "                    max(seg_y) - min(seg_y),\n",
    "                ]\n",
    "                ann[\"bbox\"] = bbox\n",
    "                ann[\"segmentation\"] = [segmentation]\n",
    "                ann[\"area\"] = area\n",
    "                ann[\"image_id\"] = image_id\n",
    "                ann[\"iscrowd\"] = 0\n",
    "                ann[\"id\"] = annot_id\n",
    "                annot_id += 1\n",
    "            if ann:\n",
    "                annotations.append(ann)\n",
    "        image_id += 1\n",
    "    merged_data = {\n",
    "        \"images\": images,\n",
    "        \"annotations\": annotations,\n",
    "        \"categories\": [\n",
    "            {\"supercategory\": \"lane\", \"id\": 1, \"name\": \"lane_blue\"},\n",
    "            {\"supercategory\": \"lane\", \"id\": 2, \"name\": \"lane_shoulder\"},\n",
    "            {\"supercategory\": \"lane\", \"id\": 3, \"name\": \"lane_white\"},\n",
    "            {\"supercategory\": \"lane\", \"id\": 4, \"name\": \"lane_yellow\"},\n",
    "        ],\n",
    "    }\n",
    "    with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(merged_data, f, ensure_ascii=False, indent=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705288b6-a31b-4a8b-a6da-5dc6e9fd6565",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-09 17:09:59 \u001b[32mINFO     \u001b[0m 364504778.py[line:38]: Start training!!\u001b[0m\n",
      "2023-11-09 17:09:59 \u001b[32mINFO     \u001b[0m 364504778.py[line:39]: Read config from ./configs/lane_detection_config.py\u001b[0m\n",
      "2023-11-09 17:09:59 \u001b[32mINFO     \u001b[0m 364504778.py[line:45]: Merging 0 files in train dataset.\u001b[0m\n",
      "0it [00:00, ?it/s]\n",
      "2023-11-09 17:09:59 \u001b[32mINFO     \u001b[0m 364504778.py[line:45]: Merging 0 files in valid dataset.\u001b[0m\n",
      "0it [00:00, ?it/s]\n",
      "2023-11-09 17:09:59 \u001b[32mINFO     \u001b[0m 364504778.py[line:62]: Environment info:\n",
      "------------------------------------------------------------\n",
      "sys.platform: win32\n",
      "Python: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)]\n",
      "CUDA available: True\n",
      "GPU 0: NVIDIA GeForce GTX 1650\n",
      "CUDA_HOME: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.5\n",
      "NVCC: Not Available\n",
      "GCC: n/a\n",
      "PyTorch: 1.7.0+cu110\n",
      "PyTorch compiling details: PyTorch built with:\n",
      "  - C++ Version: 199711\n",
      "  - MSVC 192729112\n",
      "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191125 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)\n",
      "  - OpenMP 2019\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.0\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_37,code=compute_37\n",
      "  - CuDNN 8.0.4\n",
      "  - Magma 2.5.3\n",
      "  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS=/DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj -openmp:experimental -DNDEBUG -DUSE_FBGEMM -DUSE_VULKAN_WRAPPER, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=OFF, USE_OPENMP=ON, \n",
      "\n",
      "TorchVision: 0.8.1+cu110\n",
      "OpenCV: 4.8.1\n",
      "MMCV: 1.4.0\n",
      "MMCV Compiler: MSVC 192930152\n",
      "MMCV CUDA Compiler: 11.5\n",
      "MMDetection: 2.18.1+unknown\n",
      "------------------------------------------------------------\n",
      "\u001b[0m\n",
      "2023-11-09 17:10:00 \u001b[32mINFO     \u001b[0m 364504778.py[line:66]: Distributed training: False\u001b[0m\n",
      "2023-11-09 17:10:00 \u001b[32mINFO     \u001b[0m 364504778.py[line:67]: Config:\n",
      "model = dict(\n",
      "    type='MaskRCNN',\n",
      "    backbone=dict(\n",
      "        type='ResNet',\n",
      "        depth=101,\n",
      "        num_stages=4,\n",
      "        out_indices=(0, 1, 2, 3),\n",
      "        frozen_stages=-1,\n",
      "        norm_cfg=dict(type='BN', requires_grad=True),\n",
      "        norm_eval=True,\n",
      "        style='pytorch',\n",
      "        init_cfg=dict(type='Pretrained',\n",
      "                      checkpoint='torchvision://resnet101')),\n",
      "    neck=dict(\n",
      "        type='FPN_CARAFE',\n",
      "        in_channels=[256, 512, 1024, 2048],\n",
      "        out_channels=256,\n",
      "        num_outs=5,\n",
      "        start_level=0,\n",
      "        end_level=-1,\n",
      "        norm_cfg=None,\n",
      "        act_cfg=None,\n",
      "        order=('conv', 'norm', 'act'),\n",
      "        upsample_cfg=dict(\n",
      "            type='carafe',\n",
      "            up_kernel=7,\n",
      "            up_group=1,\n",
      "            encoder_kernel=5,\n",
      "            encoder_dilation=1,\n",
      "            compressed_channels=32)),\n",
      "    rpn_head=dict(\n",
      "        type='RPNHead',\n",
      "        in_channels=256,\n",
      "        feat_channels=256,\n",
      "        anchor_generator=dict(\n",
      "            type='AnchorGenerator',\n",
      "            scales=[8],\n",
      "            ratios=[0.5, 1.0, 2.0],\n",
      "            strides=[4, 8, 16, 32, 64]),\n",
      "        bbox_coder=dict(\n",
      "            type='DeltaXYWHBBoxCoder',\n",
      "            target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "            target_stds=[1.0, 1.0, 1.0, 1.0]),\n",
      "        loss_cls=dict(\n",
      "            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n",
      "        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),\n",
      "    roi_head=dict(\n",
      "        type='StandardRoIHead',\n",
      "        bbox_roi_extractor=dict(\n",
      "            type='GenericRoIExtractor',\n",
      "            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=2),\n",
      "            out_channels=256,\n",
      "            featmap_strides=[4, 8, 16, 32],\n",
      "            aggregation='sum',\n",
      "            pre_cfg=dict(\n",
      "                type='ConvModule',\n",
      "                in_channels=256,\n",
      "                out_channels=256,\n",
      "                kernel_size=5,\n",
      "                padding=2,\n",
      "                inplace=False),\n",
      "            post_cfg=dict(\n",
      "                type='GeneralizedAttention',\n",
      "                in_channels=256,\n",
      "                spatial_range=-1,\n",
      "                num_heads=6,\n",
      "                attention_type='0100',\n",
      "                kv_stride=2)),\n",
      "        bbox_head=dict(\n",
      "            type='Shared2FCBBoxHead',\n",
      "            in_channels=256,\n",
      "            fc_out_channels=1024,\n",
      "            roi_feat_size=7,\n",
      "            num_classes=4,\n",
      "            bbox_coder=dict(\n",
      "                type='DeltaXYWHBBoxCoder',\n",
      "                target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                target_stds=[0.1, 0.1, 0.2, 0.2]),\n",
      "            reg_class_agnostic=False,\n",
      "            loss_cls=dict(\n",
      "                type='SeesawLoss',\n",
      "                use_sigmoid=False,\n",
      "                loss_weight=1.0,\n",
      "                p=0.8,\n",
      "                q=2.0,\n",
      "                num_classes=4),\n",
      "            loss_bbox=dict(type='L1Loss', loss_weight=1.0)),\n",
      "        mask_roi_extractor=dict(\n",
      "            type='GenericRoIExtractor',\n",
      "            roi_layer=dict(type='RoIAlign', output_size=14, sampling_ratio=2),\n",
      "            out_channels=256,\n",
      "            featmap_strides=[4, 8, 16, 32],\n",
      "            pre_cfg=dict(\n",
      "                type='ConvModule',\n",
      "                in_channels=256,\n",
      "                out_channels=256,\n",
      "                kernel_size=5,\n",
      "                padding=2,\n",
      "                inplace=False),\n",
      "            post_cfg=dict(\n",
      "                type='GeneralizedAttention',\n",
      "                in_channels=256,\n",
      "                spatial_range=-1,\n",
      "                num_heads=6,\n",
      "                attention_type='0100',\n",
      "                kv_stride=2)),\n",
      "        mask_head=dict(\n",
      "            type='FCNMaskHead',\n",
      "            num_convs=4,\n",
      "            in_channels=256,\n",
      "            conv_out_channels=256,\n",
      "            num_classes=4,\n",
      "            loss_mask=dict(\n",
      "                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0),\n",
      "            upsample_cfg=dict(\n",
      "                type='carafe',\n",
      "                scale_factor=2,\n",
      "                up_kernel=7,\n",
      "                up_group=1,\n",
      "                encoder_kernel=5,\n",
      "                encoder_dilation=1,\n",
      "                compressed_channels=32),\n",
      "            predictor_cfg=dict(type='NormedConv2d', tempearture=20))),\n",
      "    train_cfg=dict(\n",
      "        rpn=dict(\n",
      "            assigner=dict(\n",
      "                type='MaxIoUAssigner',\n",
      "                pos_iou_thr=0.7,\n",
      "                neg_iou_thr=0.3,\n",
      "                min_pos_iou=0.3,\n",
      "                match_low_quality=True,\n",
      "                ignore_iof_thr=-1),\n",
      "            sampler=dict(\n",
      "                type='RandomSampler',\n",
      "                num=256,\n",
      "                pos_fraction=0.5,\n",
      "                neg_pos_ub=-1,\n",
      "                add_gt_as_proposals=False),\n",
      "            allowed_border=-1,\n",
      "            pos_weight=-1,\n",
      "            debug=False),\n",
      "        rpn_proposal=dict(\n",
      "            nms_pre=2000,\n",
      "            max_per_img=1000,\n",
      "            nms=dict(type='nms', iou_threshold=0.7),\n",
      "            min_bbox_size=0),\n",
      "        rcnn=dict(\n",
      "            assigner=dict(\n",
      "                type='MaxIoUAssigner',\n",
      "                pos_iou_thr=0.5,\n",
      "                neg_iou_thr=0.5,\n",
      "                min_pos_iou=0.5,\n",
      "                match_low_quality=True,\n",
      "                ignore_iof_thr=-1),\n",
      "            sampler=dict(\n",
      "                type='RandomSampler',\n",
      "                num=512,\n",
      "                pos_fraction=0.25,\n",
      "                neg_pos_ub=-1,\n",
      "                add_gt_as_proposals=True),\n",
      "            mask_size=28,\n",
      "            pos_weight=-1,\n",
      "            debug=False)),\n",
      "    test_cfg=dict(\n",
      "        rpn=dict(\n",
      "            nms_pre=1000,\n",
      "            max_per_img=1000,\n",
      "            nms=dict(type='nms', iou_threshold=0.7),\n",
      "            min_bbox_size=0),\n",
      "        rcnn=dict(\n",
      "            score_thr=0.05,\n",
      "            nms=dict(type='nms', iou_threshold=0.5),\n",
      "            max_per_img=100,\n",
      "            mask_thr_binary=0.5)))\n",
      "dataset_type = 'CocoDataset'\n",
      "data_root = 'data/coco/'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[105.685, 99.015, 101.624], std=[65.58, 65.665, 67.324], to_rgb=True)\n",
      "train_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n",
      "    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n",
      "    dict(type='RandomFlip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[105.685, 99.015, 101.624],\n",
      "        std=[65.58, 65.665, 67.324],\n",
      "        to_rgb=True),\n",
      "    dict(type='Pad', size_divisor=32),\n",
      "    dict(type='DefaultFormatBundle'),\n",
      "    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(\n",
      "        type='MultiScaleFlipAug',\n",
      "        img_scale=(1333, 800),\n",
      "        flip=False,\n",
      "        transforms=[\n",
      "            dict(type='Resize', keep_ratio=True),\n",
      "            dict(type='RandomFlip'),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[105.685, 99.015, 101.624],\n",
      "                std=[65.58, 65.665, 67.324],\n",
      "                to_rgb=True),\n",
      "            dict(type='Pad', size_divisor=32),\n",
      "            dict(type='ImageToTensor', keys=['img']),\n",
      "            dict(type='Collect', keys=['img'])\n",
      "        ])\n",
      "]\n",
      "data = dict(\n",
      "    samples_per_gpu=4,\n",
      "    workers_per_gpu=8,\n",
      "    train=dict(\n",
      "        type='CocoDataset',\n",
      "        ann_file='/data/lane_cocostyle/train.json',\n",
      "        img_prefix='/data/train/IMAGE',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n",
      "            dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n",
      "            dict(type='RandomFlip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[105.685, 99.015, 101.624],\n",
      "                std=[65.58, 65.665, 67.324],\n",
      "                to_rgb=True),\n",
      "            dict(type='Pad', size_divisor=32),\n",
      "            dict(type='DefaultFormatBundle'),\n",
      "            dict(\n",
      "                type='Collect',\n",
      "                keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])\n",
      "        ],\n",
      "        classes=('lane_blue', 'lane_shoulder', 'lane_white', 'lane_yellow')),\n",
      "    val=dict(\n",
      "        type='CocoDataset',\n",
      "        ann_file='/data/lane_cocostyle/valid.json',\n",
      "        img_prefix='/data/valid/IMAGE',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(1333, 800),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[105.685, 99.015, 101.624],\n",
      "                        std=[65.58, 65.665, 67.324],\n",
      "                        to_rgb=True),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ],\n",
      "        classes=('lane_blue', 'lane_shoulder', 'lane_white', 'lane_yellow')),\n",
      "    test=dict(\n",
      "        type='CocoDataset',\n",
      "        ann_file='/data/lane_cocostyle/test.json',\n",
      "        img_prefix='/data/test/IMAGE',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(1333, 800),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[105.685, 99.015, 101.624],\n",
      "                        std=[65.58, 65.665, 67.324],\n",
      "                        to_rgb=True),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ],\n",
      "        classes=('lane_blue', 'lane_shoulder', 'lane_white', 'lane_yellow')))\n",
      "evaluation = dict(metric=['bbox', 'segm'], interval=500, save_best='segm_mAP')\n",
      "optimizer_config = dict(grad_clip=None)\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    warmup='linear',\n",
      "    warmup_iters=500,\n",
      "    warmup_ratio=0.001,\n",
      "    step=[400000, 500000],\n",
      "    min_lr=1e-07)\n",
      "checkpoint_config = dict(interval=500, max_keep_ckpts=3)\n",
      "log_config = dict(\n",
      "    interval=50,\n",
      "    hooks=[dict(type='TextLoggerHook'),\n",
      "           dict(type='TensorboardLoggerHook')])\n",
      "custom_hooks = [dict(type='NumClassCheckHook')]\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = ''\n",
      "resume_from = './pretrained/pretrained_lane_model.pth'\n",
      "workflow = [('train', 1)]\n",
      "gpu_ids = [0]\n",
      "seed = 1111\n",
      "work_dir = './trained_models/lane_detection'\n",
      "cudnn_benchmark = True\n",
      "eval_interval = 500\n",
      "runner = dict(type='IterBasedRunner', max_iters=600000)\n",
      "optimizer = dict(type='Adam', lr=0.0003, weight_decay=0.0001)\n",
      "data_source = '/data/'\n",
      "ann_source = '/data/lane_cocostyle/'\n",
      "classes = ('lane_blue', 'lane_shoulder', 'lane_white', 'lane_yellow')\n",
      "train_data_set = dict(\n",
      "    type='CocoDataset',\n",
      "    img_prefix='/data/train/IMAGE',\n",
      "    classes=('lane_blue', 'lane_shoulder', 'lane_white', 'lane_yellow'),\n",
      "    ann_file='/data/lane_cocostyle/train.json',\n",
      "    pipeline=[\n",
      "        dict(type='LoadImageFromFile'),\n",
      "        dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n",
      "        dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n",
      "        dict(type='RandomFlip', flip_ratio=0.5),\n",
      "        dict(\n",
      "            type='Normalize',\n",
      "            mean=[105.685, 99.015, 101.624],\n",
      "            std=[65.58, 65.665, 67.324],\n",
      "            to_rgb=True),\n",
      "        dict(type='Pad', size_divisor=32),\n",
      "        dict(type='DefaultFormatBundle'),\n",
      "        dict(\n",
      "            type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])\n",
      "    ])\n",
      "valid_data_set = dict(\n",
      "    type='CocoDataset',\n",
      "    img_prefix='/data/valid/IMAGE',\n",
      "    classes=('lane_blue', 'lane_shoulder', 'lane_white', 'lane_yellow'),\n",
      "    ann_file='/data/lane_cocostyle/valid.json',\n",
      "    pipeline=[\n",
      "        dict(type='LoadImageFromFile'),\n",
      "        dict(\n",
      "            type='MultiScaleFlipAug',\n",
      "            img_scale=(1333, 800),\n",
      "            flip=False,\n",
      "            transforms=[\n",
      "                dict(type='Resize', keep_ratio=True),\n",
      "                dict(type='RandomFlip'),\n",
      "                dict(\n",
      "                    type='Normalize',\n",
      "                    mean=[105.685, 99.015, 101.624],\n",
      "                    std=[65.58, 65.665, 67.324],\n",
      "                    to_rgb=True),\n",
      "                dict(type='Pad', size_divisor=32),\n",
      "                dict(type='ImageToTensor', keys=['img']),\n",
      "                dict(type='Collect', keys=['img'])\n",
      "            ])\n",
      "    ])\n",
      "test_data_set = dict(\n",
      "    type='CocoDataset',\n",
      "    img_prefix='/data/test/IMAGE',\n",
      "    classes=('lane_blue', 'lane_shoulder', 'lane_white', 'lane_yellow'),\n",
      "    ann_file='/data/lane_cocostyle/test.json',\n",
      "    pipeline=[\n",
      "        dict(type='LoadImageFromFile'),\n",
      "        dict(\n",
      "            type='MultiScaleFlipAug',\n",
      "            img_scale=(1333, 800),\n",
      "            flip=False,\n",
      "            transforms=[\n",
      "                dict(type='Resize', keep_ratio=True),\n",
      "                dict(type='RandomFlip'),\n",
      "                dict(\n",
      "                    type='Normalize',\n",
      "                    mean=[105.685, 99.015, 101.624],\n",
      "                    std=[65.58, 65.665, 67.324],\n",
      "                    to_rgb=True),\n",
      "                dict(type='Pad', size_divisor=32),\n",
      "                dict(type='ImageToTensor', keys=['img']),\n",
      "                dict(type='Collect', keys=['img'])\n",
      "            ])\n",
      "    ])\n",
      "\u001b[0m\n",
      "2023-11-09 17:10:00 \u001b[32mINFO     \u001b[0m 364504778.py[line:71]: Set random seed to 1111, deterministic: False\u001b[0m\n",
      "2023-11-09 17:10:01,209 - mmcv - INFO - initialize ResNet with init_cfg {'type': 'Pretrained', 'checkpoint': 'torchvision://resnet101'}\n",
      "2023-11-09 17:10:01,218 - mmcv - INFO - load model from: torchvision://resnet101\n",
      "2023-11-09 17:10:01,218 - mmcv - INFO - load checkpoint from torchvision path: torchvision://resnet101\n",
      "Downloading: \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\" to C:\\Users\\user/.cache\\torch\\hub\\checkpoints\\resnet101-5d3b4d8f.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc32b5f6e2204e3398319a1a5cba7508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/170M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-09 17:10:08,203 - mmcv - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: fc.weight, fc.bias\n",
      "\n",
      "2023-11-09 17:10:08,368 - mmcv - INFO - initialize RPNHead with init_cfg {'type': 'Normal', 'layer': 'Conv2d', 'std': 0.01}\n",
      "2023-11-09 17:10:08,386 - mmcv - INFO - initialize Shared2FCBBoxHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}, {'type': 'Xavier', 'override': [{'name': 'shared_fcs'}, {'name': 'cls_fcs'}, {'name': 'reg_fcs'}]}]\n",
      "2023-11-09 17:10:08,513 - mmcv - INFO - \n",
      "backbone.conv1.weight - torch.Size([64, 3, 7, 7]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,513 - mmcv - INFO - \n",
      "backbone.bn1.weight - torch.Size([64]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,513 - mmcv - INFO - \n",
      "backbone.bn1.bias - torch.Size([64]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,519 - mmcv - INFO - \n",
      "backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,519 - mmcv - INFO - \n",
      "backbone.layer1.0.bn1.weight - torch.Size([64]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,519 - mmcv - INFO - \n",
      "backbone.layer1.0.bn1.bias - torch.Size([64]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,520 - mmcv - INFO - \n",
      "backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,520 - mmcv - INFO - \n",
      "backbone.layer1.0.bn2.weight - torch.Size([64]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,521 - mmcv - INFO - \n",
      "backbone.layer1.0.bn2.bias - torch.Size([64]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,521 - mmcv - INFO - \n",
      "backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,522 - mmcv - INFO - \n",
      "backbone.layer1.0.bn3.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,522 - mmcv - INFO - \n",
      "backbone.layer1.0.bn3.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,522 - mmcv - INFO - \n",
      "backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,522 - mmcv - INFO - \n",
      "backbone.layer1.0.downsample.1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,522 - mmcv - INFO - \n",
      "backbone.layer1.0.downsample.1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,522 - mmcv - INFO - \n",
      "backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,522 - mmcv - INFO - \n",
      "backbone.layer1.1.bn1.weight - torch.Size([64]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,522 - mmcv - INFO - \n",
      "backbone.layer1.1.bn1.bias - torch.Size([64]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,522 - mmcv - INFO - \n",
      "backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,522 - mmcv - INFO - \n",
      "backbone.layer1.1.bn2.weight - torch.Size([64]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,522 - mmcv - INFO - \n",
      "backbone.layer1.1.bn2.bias - torch.Size([64]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,522 - mmcv - INFO - \n",
      "backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,522 - mmcv - INFO - \n",
      "backbone.layer1.1.bn3.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,522 - mmcv - INFO - \n",
      "backbone.layer1.1.bn3.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,522 - mmcv - INFO - \n",
      "backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,522 - mmcv - INFO - \n",
      "backbone.layer1.2.bn1.weight - torch.Size([64]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,522 - mmcv - INFO - \n",
      "backbone.layer1.2.bn1.bias - torch.Size([64]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,522 - mmcv - INFO - \n",
      "backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,522 - mmcv - INFO - \n",
      "backbone.layer1.2.bn2.weight - torch.Size([64]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,522 - mmcv - INFO - \n",
      "backbone.layer1.2.bn2.bias - torch.Size([64]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,522 - mmcv - INFO - \n",
      "backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,522 - mmcv - INFO - \n",
      "backbone.layer1.2.bn3.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,522 - mmcv - INFO - \n",
      "backbone.layer1.2.bn3.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,522 - mmcv - INFO - \n",
      "backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,522 - mmcv - INFO - \n",
      "backbone.layer2.0.bn1.weight - torch.Size([128]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,522 - mmcv - INFO - \n",
      "backbone.layer2.0.bn1.bias - torch.Size([128]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,522 - mmcv - INFO - \n",
      "backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,522 - mmcv - INFO - \n",
      "backbone.layer2.0.bn2.weight - torch.Size([128]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,522 - mmcv - INFO - \n",
      "backbone.layer2.0.bn2.bias - torch.Size([128]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,522 - mmcv - INFO - \n",
      "backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,522 - mmcv - INFO - \n",
      "backbone.layer2.0.bn3.weight - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,522 - mmcv - INFO - \n",
      "backbone.layer2.0.bn3.bias - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,522 - mmcv - INFO - \n",
      "backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,534 - mmcv - INFO - \n",
      "backbone.layer2.0.downsample.1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,534 - mmcv - INFO - \n",
      "backbone.layer2.0.downsample.1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,535 - mmcv - INFO - \n",
      "backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,535 - mmcv - INFO - \n",
      "backbone.layer2.1.bn1.weight - torch.Size([128]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,535 - mmcv - INFO - \n",
      "backbone.layer2.1.bn1.bias - torch.Size([128]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,536 - mmcv - INFO - \n",
      "backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,536 - mmcv - INFO - \n",
      "backbone.layer2.1.bn2.weight - torch.Size([128]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,537 - mmcv - INFO - \n",
      "backbone.layer2.1.bn2.bias - torch.Size([128]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,537 - mmcv - INFO - \n",
      "backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,537 - mmcv - INFO - \n",
      "backbone.layer2.1.bn3.weight - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,538 - mmcv - INFO - \n",
      "backbone.layer2.1.bn3.bias - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,538 - mmcv - INFO - \n",
      "backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,539 - mmcv - INFO - \n",
      "backbone.layer2.2.bn1.weight - torch.Size([128]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,539 - mmcv - INFO - \n",
      "backbone.layer2.2.bn1.bias - torch.Size([128]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,539 - mmcv - INFO - \n",
      "backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,540 - mmcv - INFO - \n",
      "backbone.layer2.2.bn2.weight - torch.Size([128]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,540 - mmcv - INFO - \n",
      "backbone.layer2.2.bn2.bias - torch.Size([128]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,540 - mmcv - INFO - \n",
      "backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,541 - mmcv - INFO - \n",
      "backbone.layer2.2.bn3.weight - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,541 - mmcv - INFO - \n",
      "backbone.layer2.2.bn3.bias - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,542 - mmcv - INFO - \n",
      "backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,542 - mmcv - INFO - \n",
      "backbone.layer2.3.bn1.weight - torch.Size([128]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,542 - mmcv - INFO - \n",
      "backbone.layer2.3.bn1.bias - torch.Size([128]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,543 - mmcv - INFO - \n",
      "backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,543 - mmcv - INFO - \n",
      "backbone.layer2.3.bn2.weight - torch.Size([128]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,544 - mmcv - INFO - \n",
      "backbone.layer2.3.bn2.bias - torch.Size([128]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,544 - mmcv - INFO - \n",
      "backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,544 - mmcv - INFO - \n",
      "backbone.layer2.3.bn3.weight - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,545 - mmcv - INFO - \n",
      "backbone.layer2.3.bn3.bias - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,545 - mmcv - INFO - \n",
      "backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,545 - mmcv - INFO - \n",
      "backbone.layer3.0.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,546 - mmcv - INFO - \n",
      "backbone.layer3.0.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,546 - mmcv - INFO - \n",
      "backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,547 - mmcv - INFO - \n",
      "backbone.layer3.0.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,547 - mmcv - INFO - \n",
      "backbone.layer3.0.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,547 - mmcv - INFO - \n",
      "backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,548 - mmcv - INFO - \n",
      "backbone.layer3.0.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,548 - mmcv - INFO - \n",
      "backbone.layer3.0.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,548 - mmcv - INFO - \n",
      "backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,549 - mmcv - INFO - \n",
      "backbone.layer3.0.downsample.1.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,549 - mmcv - INFO - \n",
      "backbone.layer3.0.downsample.1.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,550 - mmcv - INFO - \n",
      "backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,550 - mmcv - INFO - \n",
      "backbone.layer3.1.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,551 - mmcv - INFO - \n",
      "backbone.layer3.1.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,551 - mmcv - INFO - \n",
      "backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,551 - mmcv - INFO - \n",
      "backbone.layer3.1.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,552 - mmcv - INFO - \n",
      "backbone.layer3.1.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,552 - mmcv - INFO - \n",
      "backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,553 - mmcv - INFO - \n",
      "backbone.layer3.1.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,553 - mmcv - INFO - \n",
      "backbone.layer3.1.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,553 - mmcv - INFO - \n",
      "backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,554 - mmcv - INFO - \n",
      "backbone.layer3.2.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,554 - mmcv - INFO - \n",
      "backbone.layer3.2.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,554 - mmcv - INFO - \n",
      "backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,555 - mmcv - INFO - \n",
      "backbone.layer3.2.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,555 - mmcv - INFO - \n",
      "backbone.layer3.2.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,556 - mmcv - INFO - \n",
      "backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,556 - mmcv - INFO - \n",
      "backbone.layer3.2.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,556 - mmcv - INFO - \n",
      "backbone.layer3.2.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,557 - mmcv - INFO - \n",
      "backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,557 - mmcv - INFO - \n",
      "backbone.layer3.3.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,558 - mmcv - INFO - \n",
      "backbone.layer3.3.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,558 - mmcv - INFO - \n",
      "backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,558 - mmcv - INFO - \n",
      "backbone.layer3.3.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,559 - mmcv - INFO - \n",
      "backbone.layer3.3.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,559 - mmcv - INFO - \n",
      "backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,559 - mmcv - INFO - \n",
      "backbone.layer3.3.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,560 - mmcv - INFO - \n",
      "backbone.layer3.3.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,560 - mmcv - INFO - \n",
      "backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,560 - mmcv - INFO - \n",
      "backbone.layer3.4.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,561 - mmcv - INFO - \n",
      "backbone.layer3.4.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,562 - mmcv - INFO - \n",
      "backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,562 - mmcv - INFO - \n",
      "backbone.layer3.4.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,562 - mmcv - INFO - \n",
      "backbone.layer3.4.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,563 - mmcv - INFO - \n",
      "backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,563 - mmcv - INFO - \n",
      "backbone.layer3.4.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,563 - mmcv - INFO - \n",
      "backbone.layer3.4.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,563 - mmcv - INFO - \n",
      "backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,563 - mmcv - INFO - \n",
      "backbone.layer3.5.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,563 - mmcv - INFO - \n",
      "backbone.layer3.5.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,563 - mmcv - INFO - \n",
      "backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,563 - mmcv - INFO - \n",
      "backbone.layer3.5.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,563 - mmcv - INFO - \n",
      "backbone.layer3.5.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,563 - mmcv - INFO - \n",
      "backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,563 - mmcv - INFO - \n",
      "backbone.layer3.5.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,563 - mmcv - INFO - \n",
      "backbone.layer3.5.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,568 - mmcv - INFO - \n",
      "backbone.layer3.6.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,568 - mmcv - INFO - \n",
      "backbone.layer3.6.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,568 - mmcv - INFO - \n",
      "backbone.layer3.6.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,569 - mmcv - INFO - \n",
      "backbone.layer3.6.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,569 - mmcv - INFO - \n",
      "backbone.layer3.6.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,569 - mmcv - INFO - \n",
      "backbone.layer3.6.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,570 - mmcv - INFO - \n",
      "backbone.layer3.6.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,570 - mmcv - INFO - \n",
      "backbone.layer3.6.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,570 - mmcv - INFO - \n",
      "backbone.layer3.6.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,571 - mmcv - INFO - \n",
      "backbone.layer3.7.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,571 - mmcv - INFO - \n",
      "backbone.layer3.7.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,572 - mmcv - INFO - \n",
      "backbone.layer3.7.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,572 - mmcv - INFO - \n",
      "backbone.layer3.7.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,572 - mmcv - INFO - \n",
      "backbone.layer3.7.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,573 - mmcv - INFO - \n",
      "backbone.layer3.7.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,573 - mmcv - INFO - \n",
      "backbone.layer3.7.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,574 - mmcv - INFO - \n",
      "backbone.layer3.7.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,574 - mmcv - INFO - \n",
      "backbone.layer3.7.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,574 - mmcv - INFO - \n",
      "backbone.layer3.8.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,575 - mmcv - INFO - \n",
      "backbone.layer3.8.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,575 - mmcv - INFO - \n",
      "backbone.layer3.8.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,575 - mmcv - INFO - \n",
      "backbone.layer3.8.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,576 - mmcv - INFO - \n",
      "backbone.layer3.8.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,576 - mmcv - INFO - \n",
      "backbone.layer3.8.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,576 - mmcv - INFO - \n",
      "backbone.layer3.8.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,577 - mmcv - INFO - \n",
      "backbone.layer3.8.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,577 - mmcv - INFO - \n",
      "backbone.layer3.8.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,578 - mmcv - INFO - \n",
      "backbone.layer3.9.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,578 - mmcv - INFO - \n",
      "backbone.layer3.9.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,578 - mmcv - INFO - \n",
      "backbone.layer3.9.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,579 - mmcv - INFO - \n",
      "backbone.layer3.9.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,579 - mmcv - INFO - \n",
      "backbone.layer3.9.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,580 - mmcv - INFO - \n",
      "backbone.layer3.9.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,580 - mmcv - INFO - \n",
      "backbone.layer3.9.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,580 - mmcv - INFO - \n",
      "backbone.layer3.9.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,581 - mmcv - INFO - \n",
      "backbone.layer3.9.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,581 - mmcv - INFO - \n",
      "backbone.layer3.10.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,581 - mmcv - INFO - \n",
      "backbone.layer3.10.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,582 - mmcv - INFO - \n",
      "backbone.layer3.10.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,582 - mmcv - INFO - \n",
      "backbone.layer3.10.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,583 - mmcv - INFO - \n",
      "backbone.layer3.10.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,583 - mmcv - INFO - \n",
      "backbone.layer3.10.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,583 - mmcv - INFO - \n",
      "backbone.layer3.10.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,584 - mmcv - INFO - \n",
      "backbone.layer3.10.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,584 - mmcv - INFO - \n",
      "backbone.layer3.10.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,585 - mmcv - INFO - \n",
      "backbone.layer3.11.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,585 - mmcv - INFO - \n",
      "backbone.layer3.11.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,586 - mmcv - INFO - \n",
      "backbone.layer3.11.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,586 - mmcv - INFO - \n",
      "backbone.layer3.11.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,586 - mmcv - INFO - \n",
      "backbone.layer3.11.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,586 - mmcv - INFO - \n",
      "backbone.layer3.11.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,586 - mmcv - INFO - \n",
      "backbone.layer3.11.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,587 - mmcv - INFO - \n",
      "backbone.layer3.11.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,587 - mmcv - INFO - \n",
      "backbone.layer3.11.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,587 - mmcv - INFO - \n",
      "backbone.layer3.12.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,588 - mmcv - INFO - \n",
      "backbone.layer3.12.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,588 - mmcv - INFO - \n",
      "backbone.layer3.12.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,589 - mmcv - INFO - \n",
      "backbone.layer3.12.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,589 - mmcv - INFO - \n",
      "backbone.layer3.12.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,589 - mmcv - INFO - \n",
      "backbone.layer3.12.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,590 - mmcv - INFO - \n",
      "backbone.layer3.12.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,590 - mmcv - INFO - \n",
      "backbone.layer3.12.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,590 - mmcv - INFO - \n",
      "backbone.layer3.12.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,591 - mmcv - INFO - \n",
      "backbone.layer3.13.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,591 - mmcv - INFO - \n",
      "backbone.layer3.13.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,592 - mmcv - INFO - \n",
      "backbone.layer3.13.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,592 - mmcv - INFO - \n",
      "backbone.layer3.13.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,592 - mmcv - INFO - \n",
      "backbone.layer3.13.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,593 - mmcv - INFO - \n",
      "backbone.layer3.13.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,593 - mmcv - INFO - \n",
      "backbone.layer3.13.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,593 - mmcv - INFO - \n",
      "backbone.layer3.13.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,594 - mmcv - INFO - \n",
      "backbone.layer3.13.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,594 - mmcv - INFO - \n",
      "backbone.layer3.14.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,595 - mmcv - INFO - \n",
      "backbone.layer3.14.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,595 - mmcv - INFO - \n",
      "backbone.layer3.14.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,595 - mmcv - INFO - \n",
      "backbone.layer3.14.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,596 - mmcv - INFO - \n",
      "backbone.layer3.14.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,596 - mmcv - INFO - \n",
      "backbone.layer3.14.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,596 - mmcv - INFO - \n",
      "backbone.layer3.14.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,597 - mmcv - INFO - \n",
      "backbone.layer3.14.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,597 - mmcv - INFO - \n",
      "backbone.layer3.14.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,598 - mmcv - INFO - \n",
      "backbone.layer3.15.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,598 - mmcv - INFO - \n",
      "backbone.layer3.15.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,598 - mmcv - INFO - \n",
      "backbone.layer3.15.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,599 - mmcv - INFO - \n",
      "backbone.layer3.15.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,599 - mmcv - INFO - \n",
      "backbone.layer3.15.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,600 - mmcv - INFO - \n",
      "backbone.layer3.15.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,600 - mmcv - INFO - \n",
      "backbone.layer3.15.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,601 - mmcv - INFO - \n",
      "backbone.layer3.15.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,601 - mmcv - INFO - \n",
      "backbone.layer3.15.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,601 - mmcv - INFO - \n",
      "backbone.layer3.16.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,602 - mmcv - INFO - \n",
      "backbone.layer3.16.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,602 - mmcv - INFO - \n",
      "backbone.layer3.16.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,603 - mmcv - INFO - \n",
      "backbone.layer3.16.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,603 - mmcv - INFO - \n",
      "backbone.layer3.16.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,603 - mmcv - INFO - \n",
      "backbone.layer3.16.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,604 - mmcv - INFO - \n",
      "backbone.layer3.16.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,604 - mmcv - INFO - \n",
      "backbone.layer3.16.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,604 - mmcv - INFO - \n",
      "backbone.layer3.16.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,605 - mmcv - INFO - \n",
      "backbone.layer3.17.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,605 - mmcv - INFO - \n",
      "backbone.layer3.17.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,606 - mmcv - INFO - \n",
      "backbone.layer3.17.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,606 - mmcv - INFO - \n",
      "backbone.layer3.17.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,606 - mmcv - INFO - \n",
      "backbone.layer3.17.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,607 - mmcv - INFO - \n",
      "backbone.layer3.17.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,607 - mmcv - INFO - \n",
      "backbone.layer3.17.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,607 - mmcv - INFO - \n",
      "backbone.layer3.17.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,608 - mmcv - INFO - \n",
      "backbone.layer3.17.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,608 - mmcv - INFO - \n",
      "backbone.layer3.18.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,609 - mmcv - INFO - \n",
      "backbone.layer3.18.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,609 - mmcv - INFO - \n",
      "backbone.layer3.18.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,609 - mmcv - INFO - \n",
      "backbone.layer3.18.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,610 - mmcv - INFO - \n",
      "backbone.layer3.18.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,610 - mmcv - INFO - \n",
      "backbone.layer3.18.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,610 - mmcv - INFO - \n",
      "backbone.layer3.18.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,611 - mmcv - INFO - \n",
      "backbone.layer3.18.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,612 - mmcv - INFO - \n",
      "backbone.layer3.18.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,612 - mmcv - INFO - \n",
      "backbone.layer3.19.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,612 - mmcv - INFO - \n",
      "backbone.layer3.19.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,613 - mmcv - INFO - \n",
      "backbone.layer3.19.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,613 - mmcv - INFO - \n",
      "backbone.layer3.19.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,613 - mmcv - INFO - \n",
      "backbone.layer3.19.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,614 - mmcv - INFO - \n",
      "backbone.layer3.19.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,614 - mmcv - INFO - \n",
      "backbone.layer3.19.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,615 - mmcv - INFO - \n",
      "backbone.layer3.19.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,615 - mmcv - INFO - \n",
      "backbone.layer3.19.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,615 - mmcv - INFO - \n",
      "backbone.layer3.20.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,616 - mmcv - INFO - \n",
      "backbone.layer3.20.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,616 - mmcv - INFO - \n",
      "backbone.layer3.20.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,616 - mmcv - INFO - \n",
      "backbone.layer3.20.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,617 - mmcv - INFO - \n",
      "backbone.layer3.20.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,617 - mmcv - INFO - \n",
      "backbone.layer3.20.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,618 - mmcv - INFO - \n",
      "backbone.layer3.20.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,618 - mmcv - INFO - \n",
      "backbone.layer3.20.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,618 - mmcv - INFO - \n",
      "backbone.layer3.20.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,619 - mmcv - INFO - \n",
      "backbone.layer3.21.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,619 - mmcv - INFO - \n",
      "backbone.layer3.21.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,620 - mmcv - INFO - \n",
      "backbone.layer3.21.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,620 - mmcv - INFO - \n",
      "backbone.layer3.21.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,620 - mmcv - INFO - \n",
      "backbone.layer3.21.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,621 - mmcv - INFO - \n",
      "backbone.layer3.21.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,621 - mmcv - INFO - \n",
      "backbone.layer3.21.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,621 - mmcv - INFO - \n",
      "backbone.layer3.21.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,622 - mmcv - INFO - \n",
      "backbone.layer3.21.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,623 - mmcv - INFO - \n",
      "backbone.layer3.22.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,623 - mmcv - INFO - \n",
      "backbone.layer3.22.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,623 - mmcv - INFO - \n",
      "backbone.layer3.22.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,623 - mmcv - INFO - \n",
      "backbone.layer3.22.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,623 - mmcv - INFO - \n",
      "backbone.layer3.22.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,623 - mmcv - INFO - \n",
      "backbone.layer3.22.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,623 - mmcv - INFO - \n",
      "backbone.layer3.22.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,623 - mmcv - INFO - \n",
      "backbone.layer3.22.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,623 - mmcv - INFO - \n",
      "backbone.layer3.22.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,623 - mmcv - INFO - \n",
      "backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,623 - mmcv - INFO - \n",
      "backbone.layer4.0.bn1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,623 - mmcv - INFO - \n",
      "backbone.layer4.0.bn1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,623 - mmcv - INFO - \n",
      "backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,623 - mmcv - INFO - \n",
      "backbone.layer4.0.bn2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,623 - mmcv - INFO - \n",
      "backbone.layer4.0.bn2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,623 - mmcv - INFO - \n",
      "backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,623 - mmcv - INFO - \n",
      "backbone.layer4.0.bn3.weight - torch.Size([2048]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,623 - mmcv - INFO - \n",
      "backbone.layer4.0.bn3.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,623 - mmcv - INFO - \n",
      "backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,623 - mmcv - INFO - \n",
      "backbone.layer4.0.downsample.1.weight - torch.Size([2048]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,623 - mmcv - INFO - \n",
      "backbone.layer4.0.downsample.1.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,623 - mmcv - INFO - \n",
      "backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,623 - mmcv - INFO - \n",
      "backbone.layer4.1.bn1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,623 - mmcv - INFO - \n",
      "backbone.layer4.1.bn1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,623 - mmcv - INFO - \n",
      "backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,623 - mmcv - INFO - \n",
      "backbone.layer4.1.bn2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,623 - mmcv - INFO - \n",
      "backbone.layer4.1.bn2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,623 - mmcv - INFO - \n",
      "backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,623 - mmcv - INFO - \n",
      "backbone.layer4.1.bn3.weight - torch.Size([2048]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,623 - mmcv - INFO - \n",
      "backbone.layer4.1.bn3.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,623 - mmcv - INFO - \n",
      "backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,634 - mmcv - INFO - \n",
      "backbone.layer4.2.bn1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,634 - mmcv - INFO - \n",
      "backbone.layer4.2.bn1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,634 - mmcv - INFO - \n",
      "backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,635 - mmcv - INFO - \n",
      "backbone.layer4.2.bn2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,635 - mmcv - INFO - \n",
      "backbone.layer4.2.bn2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,636 - mmcv - INFO - \n",
      "backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,636 - mmcv - INFO - \n",
      "backbone.layer4.2.bn3.weight - torch.Size([2048]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,636 - mmcv - INFO - \n",
      "backbone.layer4.2.bn3.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from torchvision://resnet101 \n",
      " \n",
      "2023-11-09 17:10:08,638 - mmcv - INFO - \n",
      "neck.lateral_convs.0.conv.weight - torch.Size([256, 256, 1, 1]): \n",
      "Initialized by user-defined `init_weights` in FPN_CARAFE  \n",
      " \n",
      "2023-11-09 17:10:08,638 - mmcv - INFO - \n",
      "neck.lateral_convs.0.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of MaskRCNN  \n",
      " \n",
      "2023-11-09 17:10:08,638 - mmcv - INFO - \n",
      "neck.lateral_convs.1.conv.weight - torch.Size([256, 512, 1, 1]): \n",
      "Initialized by user-defined `init_weights` in FPN_CARAFE  \n",
      " \n",
      "2023-11-09 17:10:08,638 - mmcv - INFO - \n",
      "neck.lateral_convs.1.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of MaskRCNN  \n",
      " \n",
      "2023-11-09 17:10:08,638 - mmcv - INFO - \n",
      "neck.lateral_convs.2.conv.weight - torch.Size([256, 1024, 1, 1]): \n",
      "Initialized by user-defined `init_weights` in FPN_CARAFE  \n",
      " \n",
      "2023-11-09 17:10:08,639 - mmcv - INFO - \n",
      "neck.lateral_convs.2.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of MaskRCNN  \n",
      " \n",
      "2023-11-09 17:10:08,639 - mmcv - INFO - \n",
      "neck.lateral_convs.3.conv.weight - torch.Size([256, 2048, 1, 1]): \n",
      "Initialized by user-defined `init_weights` in FPN_CARAFE  \n",
      " \n",
      "2023-11-09 17:10:08,640 - mmcv - INFO - \n",
      "neck.lateral_convs.3.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of MaskRCNN  \n",
      " \n",
      "2023-11-09 17:10:08,640 - mmcv - INFO - \n",
      "neck.lateral_convs.4.conv.weight - torch.Size([256, 2048, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in FPN_CARAFE  \n",
      " \n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    args = parse_args()\n",
    "    cfg = Config.fromfile(args.config)\n",
    "\n",
    "    # Set configs\n",
    "    # import modules from string list.\n",
    "    if cfg.get(\"custom_imports\", None):\n",
    "        from mmcv.utils import import_modules_from_strings\n",
    "\n",
    "        import_modules_from_strings(**cfg[\"custom_imports\"])\n",
    "    # set cudnn_benchmark\n",
    "    if cfg.get(\"cudnn_benchmark\", False):\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    if cfg.get(\"work_dir\", None) is None:\n",
    "        # use config filename as default work_dir if cfg.work_dir is None\n",
    "        cfg.work_dir = osp.join(\n",
    "            \"./work_dirs\", osp.splitext(osp.basename(args.config))[0]\n",
    "        )\n",
    "    if args.gpu_ids is not None:\n",
    "        cfg.gpu_ids = args.gpu_ids\n",
    "    # init distributed env first, since logger depends on the dist info.\n",
    "    if args.launcher == \"none\":\n",
    "        distributed = False\n",
    "    else:\n",
    "        distributed = True\n",
    "        init_dist(args.launcher, **cfg.dist_params)\n",
    "        # re-set gpu_ids with distributed training mode\n",
    "        _, world_size = get_dist_info()\n",
    "        cfg.gpu_ids = range(world_size)\n",
    "    # create work_dir\n",
    "    mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "    # dump config\n",
    "    cfg.dump(osp.join(cfg.work_dir, osp.basename(args.config)))\n",
    "    # init the logger before other steps\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())\n",
    "    log_file = osp.join(cfg.work_dir, f\"{timestamp}.log\")\n",
    "    logger = setup_logger(log_file, name=__name__)\n",
    "    logger.info(\"Start training!!\")\n",
    "    logger.info(f\"Read config from {args.config}\")\n",
    "\n",
    "    # build coco-style dataset\n",
    "    if not args.no_merging:\n",
    "        for split in [\"train\", \"valid\"]:\n",
    "            annot_list = glob(osp.join(cfg.data_source, split, \"ANNOTATION/*.json\"))\n",
    "            logger.info(f\"Merging {len(annot_list)} files in {split} dataset.\")\n",
    "            if not osp.exists(cfg.ann_source):\n",
    "                os.makedirs(cfg.ann_source)\n",
    "            save_path = osp.join(cfg.ann_source, f\"{split}.json\")\n",
    "            merge_annotation(annot_list, save_path)\n",
    "    for _, _, filename in os.walk(osp.join(cfg.data_source, \"train/IMAGE\")):\n",
    "        if len(filename) > 0:\n",
    "            logger.info(f\"Num images => {filename}\")\n",
    "            logger.info(f\"File list => {filename}\")\n",
    "\n",
    "    # init the meta dict to record some important information such as\n",
    "    # environment info and seed, which will be logged\n",
    "    meta = dict()\n",
    "    # log env info\n",
    "    env_info_dict = collect_env()\n",
    "    env_info = \"\\n\".join([(f\"{k}: {v}\") for k, v in env_info_dict.items()])\n",
    "    dash_line = \"-\" * 60 + \"\\n\"\n",
    "    logger.info(\"Environment info:\\n\" + dash_line + env_info + \"\\n\" + dash_line)\n",
    "    meta[\"env_info\"] = env_info\n",
    "    meta[\"config\"] = cfg.pretty_text\n",
    "    # log some basic info\n",
    "    logger.info(f\"Distributed training: {distributed}\")\n",
    "    logger.info(f\"Config:\\n{cfg.pretty_text}\")\n",
    "\n",
    "    # set random seeds\n",
    "    if cfg.seed is not None:\n",
    "        logger.info(\n",
    "            f\"Set random seed to {cfg.seed}, \" f\"deterministic: {args.deterministic}\"\n",
    "        )\n",
    "        set_random_seed(cfg.seed, deterministic=args.deterministic)\n",
    "    meta[\"seed\"] = cfg.seed\n",
    "    meta[\"exp_name\"] = osp.basename(args.config)\n",
    "\n",
    "    model = build_detector(\n",
    "        cfg.model, train_cfg=cfg.get(\"train_cfg\"), test_cfg=cfg.get(\"test_cfg\")\n",
    "    )\n",
    "    model.init_weights()\n",
    "\n",
    "    datasets = [build_dataset(cfg.data.train)]\n",
    "    if len(cfg.workflow) == 2:\n",
    "        val_dataset = copy.deepcopy(cfg.data.val)\n",
    "        val_dataset.pipeline = cfg.data.train.pipeline\n",
    "        datasets.append(build_dataset(val_dataset))\n",
    "    if cfg.checkpoint_config is not None:\n",
    "        # save mmdet version, config file content and class names in\n",
    "        # checkpoints as meta data\n",
    "        cfg.checkpoint_config.meta = dict(\n",
    "            mmdet_version=__version__ + get_git_hash()[:7], CLASSES=datasets[0].CLASSES\n",
    "        )\n",
    "    # add an attribute for visualization convenience\n",
    "    model.CLASSES = datasets[0].CLASSES\n",
    "    train_detector(\n",
    "        model,\n",
    "        datasets,\n",
    "        cfg,\n",
    "        distributed=distributed,\n",
    "        validate=(not args.no_validate),\n",
    "        timestamp=timestamp,\n",
    "        meta=meta,\n",
    "    )\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec525ed-2320-4d0c-a1e4-159eedbb9c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install yapf==0.40.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5599139a-acc1-47e7-9592-67b075fe7657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e30696-0de7-4940-83c6-d59c08d0a589",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
